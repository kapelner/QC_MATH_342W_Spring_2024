---
title: "Practice Lecture 14 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


# Logistic Regression for Binary Response

Let's clean up and load the cancer dataset, remove missing data, remove the ID column and add more appropriate feature names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've named them appropriately a few lectures ago
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
head(biopsy$class)
```

We can either estimate probability of the biopsy tissue being benign (this would mean y = 1 is the benign category level) or estimate the probability of the biopsy tissue being malignant (this would mean y = 1 is the malignant category level).

Let's go with the latter. To make the encoding explicitly 0/1, we can cast the factor to numeric or we can rely on R's default factor representation i.e. that the first level is 0 and the second level is 1. Here, we can use this default without reordering since the levels above show that benign is first and thus = 0 and malignant is second and thus = 1 (via coincidence of alphabetical order).

Now let's split into training and test for experiments:

```{r}
set.seed(1984)
K = 5
test_prop = 1 / K
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to set the family parameter to be "binomial" which means we are using the independent Bernoulli and within the binomial family, we are using the "logit" link. There are other types of family models we won't get a chance to study e.g. Poisson, negative binomial for count models

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = binomial(link = "logit"))
```

That was fast! There was actually a lot of optimization in that line. Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit the log odds of malignancy increases by 0.597...

All of the coefficients are positive which means if any of the covariates increase...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malignant. If you can read log odds, you'll see subject #... has a small probability of being malignant and subject #... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

We see lots of phats close to zero and lots close to one. The model seems very sure of itself! 

Let's see response by estimated probability another way using a box and whisker plot:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there in the training set! How about the test set?

```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = y_test), alpha = 0.5)
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Looks pretty good! 

We now will talk about error metrics for probabilistic estimation models. That will give us a way to validate this model and provide an estimate of future performance. 

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Yup can't do arithmetic operations on factors. So now we have to go ahead and cast.

```{r}
y_train_binary = ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_binary - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on. And the oos Brier score?

```{r}
y_test_binary = ifelse(y_test == "malignant", 1, 0)
mean(-(y_test_binary - p_hats_test)^2)
```

Not as good but still very good!

What is the in-sample log score?

```{r}
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
```

This isn't bad (if you get intuition in reading them). And oos?

```{r}
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Not as good but still very good!

If we wanted to be more careful, we can use K-fold CV to get a less variable oos metric. Maybe we'll do that in a lab?


# Probit and Cloglog probability estimation

These are different generalized linear models but fit using the same code. All we need to do is change the link argument. For a probit regression we just do:

```{r}
probit_mod = glm(class ~ ., biopsy_train, family = binomial(link = "probit"))
```

This is complaining about numerical underflow or overflow. If you get a z-score that's really large in magnitude, then it says probability is 1 (if z score is positive) or 0 (if z score is negative)

```{r}
coef(probit_mod)
```

As we saw before, all coefficients for the covariates are positive. What's the interpretation of b for bare_nuclei?

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(probit_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

This is basically the same. How about out of sample?


```{r}
p_hats_test = predict(probit_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Also basically the same-looking. To get an apples-apples comparison with logistic regression let's calculate the brier and log scoring metrics:

```{r}
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

It appears the logistic regression is slightly better than probit regression oos.

Let's do complementary log-log too:

```{r}
cloglog_mod = glm(class ~ ., biopsy_train, family = binomial(link = "cloglog"))
coef(cloglog_mod)
```

Same signs on coefficients. Interpretation? Difficult... 

Let's see how it does compared to the logistic and probit models.

```{r}
p_hats_train = predict(cloglog_mod, biopsy_train, type = "response")
p_hats_test = predict(cloglog_mod, biopsy_test, type = "response")
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Much worse than either! 

Logistic regression is usually the default. But just because it's the default and most popular and just because it won here doesn't mean it will always win!! Using probit or any other link function constitutes a completely different model. You can use the "model selection procedure" we will discuss to choose which link function is best for your dataset.

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year in 1994 dollars. That's the equivalent of almost $90K/yr today (see https://www.in2013dollars.com/us/inflation/1994?amount=50000).

We must load this package from github repository as it's not on CRAN.

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
?adult
#to make the exercise easier I'm going to delete two features - it's more work to get oos validation working if these two stay in which I can explain later
adult$occupation = NULL
adult$native_country = NULL
skimr::skim(adult)
```

What is g_0?

```{r}
mean(as.numeric(adult$income == ">50K"))
```

Let's use samples of 5,000 to run experiments:

```{r}
set.seed(1984)

train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial") #shortcut for binomial(link = "logit")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
length(coef(logistic_mod))
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)), alpha = 0.5)
```

Much more humble than the cancer data's model!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and only confident about the small incomes half the time. If not confident, it is predicting probs away from 0% and 100%.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
y_train_numeric = ifelse(y_train == ">50K", 1, 0)
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is indeed a major improvement.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit. This makes sense, we had $n = 5000$ and $p + 1 = 13$.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyone understands classification errors! We will see these performance metrics later in the semester.


# "Nonlinear Linear" Regression with polynomials

Even though we can demonstrate "Nonlinear Linear" Regression with polynomials using glm's for probability estimation, it is easier when the response is numeric.

Let's generate a polynomial model of degree 2 ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

```{r}
set.seed(1003)
n = 25
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n, -2, 5)
#best possible model
h_star_x = beta_0 + beta_1 * x + beta_2 * x^2

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
pacman::p_load(ggplot2)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
head(x)
```

Let's try to estimate with a line:

```{r}
linear_mod = lm(y ~ x)
b_linear = summary(linear_mod)$coef
basic + geom_abline(intercept = b_linear[1], slope = b_linear[2], col = "red")
```

The relationship is "underfit". $\mathcal{H}$ is not rich enough right now to express something close to $f(x)$. But it is better than the null model!

Now let's do a polynomial regression of degree two. Let's do so manually:

```{r}
X = as.matrix(cbind(1, x, x^2))
head(X)
b = solve(t(X) %*% X) %*% t(X) %*% y
b
c(beta_0, beta_1, beta_2)
```

These are about the same as the $\beta_0, \beta_1$ and $\beta_2$ as defined in $f(x)$ the true model. In order to graph this, we can no longer use the routine `geom_abline`, we need to use `stat_function`.

```{r}
plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}

basic + 
  stat_function(fun = plot_function_degree_2, args = list(b = b), col= "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2)), col= "darkgreen")
```

Now let's try polynomial of degree 3:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_3 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3
}

basic + stat_function(fun = plot_function_degree_3, args = list(b = b), col = "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2, 0)), col= "darkgreen")
```

Still the same. Why? The $x^3$ term is like adding one "nonsense" predictor. One nonsense predictor marginally affects $R^2$ but it doesn't matter too much.

Now let's try polynomial of degree 8:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_8 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 
}

basic + stat_function(fun = plot_function_degree_8, args = list(b = b), col = "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2, rep(0, 6))), col= "darkgreen")
```

We are seeing now a little bit of "overfitting" in the edge(s). We now have $p=9$ and $n=100$. We can do a lot worse!

Let's learn how to do this in R first without having to resort to manual linear algebra. R has a function called "poly" that can be used *inside* formula declarations.

Let's first fit the degree 2 model:

```{r}
degree_2_poly_mod = lm(y ~ poly(x, 2, raw = TRUE))
head(model.matrix(~ poly(x, 2, raw = TRUE))) #the model matrix for this regression - just to check
b_poly_2 = coef(degree_2_poly_mod)
b_poly_2
summary(degree_2_poly_mod)$r.squared
```

Let's go on a slight tangent. And look at this regression without using the raw polynomial.

```{r}
Xmm = model.matrix(~ poly(x, 2))
head(Xmm) #the model matrix for this regression - just to check
Xmm[, 1] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 3]
Xmm[, 3] %*% Xmm[, 3]
```

Are these orthogonal polynomials? How is the `poly` function without `raw = TRUE` working to generate a model matrix?

```{r}
degree_2_orthog_poly_mod = lm(y ~ poly(x, 2))
b_poly_2 = coef(degree_2_orthog_poly_mod)
b_poly_2
summary(degree_2_orthog_poly_mod)$r.squared
```

Raw or orthogonal does not affect the yhats and Rsq. They are the same as we got before! That's because the colspace is the same in both cases raw or polynomial. We use "raw" polynomials to keep them interpretable and on the same scale as the manual models we were fitting.

Now let's do polynomial of degree 13:

```{r}
degree_13_poly_mod = lm(y ~ poly(x, 13, raw = TRUE))
b_poly_13 = coef(degree_13_poly_mod)

plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

basic + stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")# + ylim(c(0, 25)) #+ xlim(c(-2, 5.2))
```

What's happening for small values of $x$ (and a bit for large values)? This is called [Runge's Phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) meaning that the boundary activity of high-order polynomials has very large derivatives. Let's go back to the same scale as before and see what's happening:

```{r}
basic + 
  coord_cartesian(xlim = c(-2, 5), ylim = c(-3, 25)) + 
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")
```

This is terrible! Future predictions will be awful at the edges and even more awful when extrapolating beyond the edges! Let's examine this extrapolation further using another dataset.


#Extrapolation vs Interpolation

Let's take a look at the Galton Data again.

```{r}
pacman::p_load(HistData, ggplot2)
data(Galton)
mod = lm(child ~ parent, Galton)
b_0 = mod$coefficients[1]
b_1 = mod$coefficients[2]
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue", size = 1) +
  xlim(63.5, 72.5) + 
  ylim(63.5, 72.5) +
  coord_equal(ratio = 1)
```

Let's say I want to predict child's height for parents' average height of 70in. All I do is:

```{r}
predict(mod, data.frame(parent = 70))
```

What if I want to predict for a parents' height of 5in. Is there any 12in tall human being? No... it is absurd. But nothing stops you from doing:

```{r}
predict(mod, data.frame(parent = 5))
```

That's [actually possible](https://www.guinnessworldrecords.com/news/2012/2/shortest-man-world-record-its-official!-chandra-bahadur-dangi-is-smallest-adult-of-all-time/).

Look at our linear model from Euclid's perspective:

```{r}
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue", size = 1) +
  xlim(-20, 120) + 
  ylim(-20, 120) +
  coord_equal(ratio = 1)
```

What is a linear model with $p + 1 = 2$. It's just a line. When geometry was first formalized by Euclid in the Elements, he defined a line to have "breadthless length" with a straight line being a line "which lies evenly with the points on itself". By "breadthless" he meant infinite in either direction. There is no mathematical problem with predicting childrens' heights using negative parents' heights e.g.

```{r}
predict(mod, data.frame(parent = -5))
```

But this is absurd. So now we need to talk about a fundamental concept in data science we've been kind of avoiding and one that most people ignore. There are two types of prediction: interpolation and extrapolation. Interpolation is essentially the type of "prediction" we've been talking about this whole class. 

Extrapolation is totally different. It's what happens when you predict outside of the range of the covariate data you've seen in $\mathbb{D}$. Extrapolation is very dangerous - your models only work based on $\mathbb{D}$. Extrapolation is prediction outside of the range you've seen before which means. You better have a good theoretical reason as to why your $\mathbb{H}$ function class will extend outside that range. Because each $\mathbb{H}$ function class will extrapolate very very differently.



What happens during extrapolation? Let's look at the (a) linear model, (b) polynomial model with degree 2 and (c) polynomial with degree 13.

```{r}
degree_2_poly_mod = lm(child ~ poly(parent, 2, raw = TRUE), Galton)
b_poly_2 = coef(degree_2_poly_mod)
degree_13_poly_mod = lm(child ~ poly(parent, 13, raw = TRUE), Galton)
b_poly_13 = coef(degree_13_poly_mod)
b_poly_13[is.na(b_poly_13)] = 0

plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}
plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

# xymin = 65
# xymax = 71
xymin = 50
xymax = 90
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue") +
  coord_cartesian(xlim = c(xymin, xymax), ylim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_2, args = list(b = b_poly_2), col = "red", xlim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "orange", xlim = c(xymin, xymax))
```

Besides being bad at the edges of the input space, polynomial models have *TERRIBLE* performance when you leave input space. The extrapolation risk is totally unpredictable due to Runge's phenomenon. Please do not use them if you think you'll ever extrapolate!!!

# Overfitting with Polynomials

Can we achieve $R^2 = 100\%$ using polynomial regression? Yes. Here's an example in one dimension. These are called "interpolation polynomials". In one dimension, as long as the $x$ values are distinct, $n$ data point can be fit by a $n - 1$ degree polynomial. Here's an example with a few data points:

```{r}
set.seed(1003)
n = 5
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n)

h_star_x = beta_0 + beta_1 * x + beta_2 * x^2
y = h_star_x + rnorm(n)
#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
```

Now fit polynomial models:

```{r}
degree_4_poly_mod = lm(y ~ poly(x, 5, raw = TRUE))
b_poly_4 = coef(degree_4_poly_mod)

plot_function_degree_4 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4
}

basic + stat_function(fun = plot_function_degree_4, args = list(b = b_poly_4), col = "purple")
```

Perfect fit!

```{r}
summary(degree_4_poly_mod)$r.squared
```

This is the same thing we've seen before! If $n = p + 1$, then the design matrix is square and there is no need to project onto a lower dimensional subspace. To estimate the linear model, one only needs to solve $n$ equations with $n$ unknowns.

My recommendations:
1) Keep polynomial degree low. Preferably 2. Anything past 2 is not interpretable anyway. We didn't talk about "interpretability" of models yet, but you get the idea. If you are using degree = 2, use raw polynomials so you get interpretability (see next section).
2) Be very careful not to extrapolate: make sure future predictions have the measurements within range of the training data $\mathbb{D}$. Extrapolations are going to be very, very inaccurate. Polynomial regressions I'm sure have gotten data scientists fired before.



# Orthogonal vs raw polynomials

Why is orthonormal polynomial the default? You can argue that doing a QR decomposition on the polynomial expansion and employing Q in the design matrix will change b thereby making b uninterpretable! So why use orthonormal polynomials? Here's why:

```{r}
n = 1000
set.seed(1984)
X = data.frame(x = c(runif(n / 2, 0, 1e-2), runif(n / 2, 0, 1e6)))
d = 10
num_digits = 8
Xmm_orth = model.matrix(~ 0 + poly(x, d), X)
colnames(Xmm_orth)[1 : d] = 1 : d
Xmm_raw = model.matrix(~ 0 + poly(x, d, raw = TRUE), X)
colnames(Xmm_raw)[1 : d] = 1 : d
```

Let's look at the design matrix for small values of x:

```{r}
head(as.matrix(X))
round(head(Xmm_orth), num_digits)
round(head(Xmm_raw), num_digits)
```

You get numerical underflow almost immediately when using the raw polynomial computations (you get it by degree 4). And thus you can't even get the OLS estimates:


```{r}
y = rnorm(n)
solve(t(Xmm_raw) %*% Xmm_raw) %*% t(Xmm_raw) %*% y
```

Let's look at the design matrix for large values of x:

```{r}
tail(as.matrix(X))
round(tail(Xmm_orth), num_digits)
round(tail(Xmm_raw), num_digits)
```

You get numerical overflow in the design matrix (but it will happen later). But the second you start to use the design matrix with 10^59's inside...

```{r}
solve(t(Xmm_raw[800 : 1000, ]) %*% Xmm_raw[800 : 1000, ]) %*% t(Xmm_raw[800 : 1000, ]) %*% y
```

As opposed to

```{r}
solve(t(Xmm_orth) %*% Xmm_orth) %*% t(Xmm_orth) %*% y
```

No problem at all!!!

So that's the reason: numerical stability. But if you need interpretability, you need raw polynomials. But if you're interpreting the model, how do you even interpret beyond degree 2???

# Log transformations

We will be examining the diamonds dataset. Let's take a moment to read about our data. In real life, you will take more than just a moment to learn about the data. And more than just a moment to clean the data and do sanity checks. As these steps are more of the grunt work of data science (and are learned on the fly), I won't really cover them formally.

```{r}
rm(list = ls())
pacman::p_load(ggplot2) #this loads the diamonds data set too
?diamonds
dim(diamonds)
```

That's a huge $n$. So, let's expect things to take a bit longer when processing.

A natural increasing relationship will likely be found between weight and price. Let's see it visually:

```{r}
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point()
```


How good does a best guess linear relationship do?

```{r}
mod = lm(price ~ carat, diamonds)
b = coef(mod)
b
summary(mod)$r.squared
summary(mod)$sigma
```

What does the intercept say about extrapolation?

Let's see the best fit line $g(x)$ visually:

```{r}
ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green") +ylim(0, 50000)
```

Maybe the relationship between weight and price is not linear - but exponential? E.g. if the weight of a diamond doubles, maybe the price quadruples? Or linear increases in weight yield percentage increases in price. This seems plausible. 

Besides the plausibility of the functional form, there is an agnostic reason to employ log y as the predictive target. Let's first examine the univariate data!


```{r}
skimr::skim(diamonds$price)
```

Very large standard error and very long tail which can be seed more clearly here:

```{r}
ggplot(diamonds) + geom_histogram(aes(price), binwidth = 200)
```

Let's take a look at the distribution after logging:

```{r}
ggplot(diamonds) + geom_histogram(aes(x = log(price)), binwidth = 0.01)
```

Some strange artifacts appear. Why the gap? Why is it "cut" sharply at a maximum? These are questions to ask the one who collected the data. But let's get back to the log story...

Popular wisdom says logging this type of highly skewed-right distribution would possibly make the model "more linear in x". Put another way, it would be easier to "catch" (predict) the long tail since it won't be a long tail anymore after you log-transform. It would also prevent observations with large y's becoming "leverage points" i.e. points that unduly influence the model and thereby warp its ability to predict the average observation. If we have time later in the semester, we can learn about leverage points, but not now. In Econ 382 you learn more reasons for why you should use log the response, but those reasons are relevant for inference so we won't discuss them here.

Let's give the model with ln(y) a whirl. Maybe we'll even learn something about diamonds. The way to create such a model is to simply fit an OLS model to log y. This is called a log-linear model. Since this is a pretty standard thing to do so R's formula notation has it built-in as follows:

```{r}
log_linear_mod = lm(log(price) ~ carat, diamonds)
b = coef(log_linear_mod)
b
```

Let's see what this looks like.

```{r}
ggplot(diamonds, aes(x = carat, y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green") + ylim(6, 12)
```

It looks very bad if carat is large. That means our little theory about carats getting larger yielding multiples of price doesn't correspond to reality.

How did we do?

```{r}
summary(log_linear_mod)$r.squared
summary(log_linear_mod)$sigma
```

Look at that RMSE! That dropped like a rock! Is that real?

No. RMSE before is in the units of y. And now y is now in ln($). So this RMSE and the previous RMSE are *not* comparable.

The $R^2$ are *not* comparable either. Even though they're on a [0, 1] scale in both models, the SST's are different so you're measuring the proportion of a different variance.

Let's attempt to compare apples-apples.

```{r}
log_y_hat = log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

What does this mean? It means this was a bad idea. Those residuals for large carats are insanely large. They're wrong on a log scale! Which means they're off by orders of magnitude. Working with logged y is dangerous business if you're wrong! Before you were off by a few thousand dollars; now you're off by millions. For example. Let's look at a large diamond:

```{r}
xstar = diamonds[diamonds$carat > 5, ][1, ]
xstar$price
predict(mod, xstar)
exp(predict(log_linear_mod, xstar))
```

That's a pretty bad residual!

How about log-log model? 

```{r}
log_log_linear_mod = lm(log(price) ~ log(carat), diamonds)
b = coef(log_log_linear_mod)
b
```

Let's see what it looks like:

```{r}
ggplot(diamonds, aes(x = log(carat), y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

Well look at that! That's a nice looking model. (Note that the slope coefficients in log-log models, i.e. b_2 here, are called "elasticity" in Econ 382 as it measures how the relative change in x affects the relative change in y).

How are our metrics?

```{r}
summary(log_log_linear_mod)$r.squared
summary(log_log_linear_mod)$sigma
```

Let's see apples-to-apples to the natural y model.

```{r}
log_y_hat = log_log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

This is on-par with the vanilla OLS model, but still doesn't "beat it". There was no guarantee that we would be "beat it" even though we used procedures that are reasonable and popular! My belief is that we really should be wary of the maximum price and employ that in the model. Maybe we'll do this in a lab exercise?

Let's repeat this entire exercise using the length of the diamond. The length of the diamond feature is confusingly named "x" in the dataset. It is an "x" but it's also the diamond's "x"!!!

```{r}
ggplot(diamonds, aes(x = x, y = price)) + 
  geom_point()
```

Besides the non-linear relationship, what else do you see? Mistakes in the dataset! Can a real diamond have zero length?? Yes. This is the real world. There are mistakes all the time.

Let's kill it! How many are we dealing with here?

```{r}
nrow(diamonds[diamonds$x == 0, ])
```


```{r}
diamonds = diamonds[diamonds$x != 0, ]
```

What's the deal with the x variable now?

```{r}
skimr::skim(diamonds$x)
```

How good does a best guess linear relationship do?

```{r}
mod = lm(price ~ x, diamonds)
b = coef(mod)
b
summary(mod)$r.squared
summary(mod)$sigma
```

Let's see the best fit line $g(x)$ visually:

```{r}
ggplot(diamonds, aes(x = x, y = price)) + geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

Again we got some bad extrapolation going on which we can't fix using a purely linear modeling strategy.

Let's log-linearize it and see how we do.

```{r}
log_linear_mod = lm(log(price) ~ x, diamonds)
b = coef(log_linear_mod)
ggplot(diamonds, aes(x = x, y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

How did we do? Ensure it's apples-apples.

```{r}
log_y_hat = log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

Still not better. Log-log?

```{r}
log_log_linear_mod = lm(log(price) ~ log(x), diamonds)
b = coef(log_log_linear_mod)
ggplot(diamonds, aes(x = log(x), y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

How did we do? 

```{r}
log_y_hat = log_log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

We did it. We found a log transformation that seems to give higher predictive power than the vanilla linear model on the raw repsonse and raw feature.

This brings up the whole idea of "model selection". We went hunting for models until we found one that's better. We will hopefully do model selection today...

Transforming y is a big decision as it changes the response metric! The rule of thumb is it is easier to model a response metric that has less extreme values (especially when using linear models) as the extreme values have a big impact on slope coefficients and can distort the best fit line due to the least squares minimization (hence the popularity of logging the response).

Let's see if we get anywhere with this using all the features in this model.

```{r}
lm_y = lm(price ~ ., diamonds)
lm_ln_y = lm(log(price) ~ ., diamonds)
summary(lm_y)$r.squared
summary(lm_y)$sigma

#now for the log-linea model
y_hat = exp(lm_ln_y$fitted.values)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
``` 

This is pretty convincing evidence that this transformation does a better job (at least in our linear modeling context).

Let's look at one prediction:

```{r}
predict(lm_y, diamonds[12345, ])
exp(predict(lm_ln_y, diamonds[12345, ]))
diamonds$price[12345]
```

Again, we should be careful when you use $g$ after logging, you will have to exponentiate the result (middle line above). 

Small point: this exponentiation is known to create bias because $E[Y]$ is different from $exp(E[ln(y)])$ (for those who took 368 - remember Jensen's inequality?) For the purposes of this class, this can be ignored since we are evaluating g on its own merits and we're doing so honestly.

If you like this stuff, there are a whole bunch of transformations out there that are even cooler than the natural log. 

# Linear Models with Feature Interactions

Let's go back to modeling price with weight. Let us add a third variable to this plot, color, a metric about the "yellowness" of the diamond. This is an ordinal categorical variable ranging from D (most clear i.e. best) to J (most yellow in this dataset i.e. worst).


```{r}
pacman::p_load(ggplot2)
base = ggplot(diamonds, aes(x = carat, y = price)) 
base +
  geom_point(aes(col = color)) + scale_color_brewer(type = "div")
```

We can split the data on color to see it more clearly:

```{r}
base +
  geom_point() +
  facet_wrap(~ color, ncol = 3) + 
  aes(color = color) + scale_color_brewer(type = "div")
```


What do we see here? It looks like the slope of the price vs. carat linear model is slightly affected by color. For instance, the "D" color diamonds' price increases much faster as weight increases than the "E" color diamonds' price increases in weight, etc. Why do you think this is?

We can picture two of these linear models below by fitting two submodels, one for D and one for J:

```{r}
mod_D = lm(price ~ carat, diamonds[diamonds$color == "D", ])
b_D = coef(mod_D)
mod_J = lm(price ~ carat, diamonds[diamonds$color == "J", ])
b_J = coef(mod_J)
b_D
b_J
```

Let's see it on the plot:

```{r}
base +
  geom_point(aes(col = color)) + scale_color_brewer(type = "div") +
  geom_abline(intercept = b_D[1], slope = b_D[2], col = "blue", lwd = 2) +
  geom_abline(intercept = b_J[1], slope = b_J[2], col = "red", lwd = 2)
```

This indicates a separate intercept and carat-slope for each color. How is this done? Interacting carat and slope. The formula notation has the `*` operator for this. It is multiplication in formula land after all!

```{r}
mod = lm(price ~ color, diamonds)
coef(mod)
mod = lm(price ~ carat * color, diamonds)
coef(mod) #beware: strange naming convention on the interaction terms for an ordered factor
diamonds$color = factor(diamonds$color, ordered = FALSE)
mod = lm(price ~ carat * color, diamonds)
coef(mod) #much better...
```

The reference category is color D. This means every other color should start lower and have a lower slope. This is about what we see above.

How much of a better model is this than a straight linear model?

```{r}
mod_vanilla = lm(price ~ carat + color, diamonds)
summary(mod_vanilla)$r.squared
summary(mod_vanilla)$sigma
summary(mod)$r.squared
summary(mod)$sigma
```

You can get more predictive accuracy out of this. We added a degree of freedom? Is this gain real? Yes. With one more feature and $n = 54,000$ there is no chance this gain came from overfitting noise. Add 10,000 garbage features, yes, there will be overgitting.

Let's take a look at carat with another variable, depth, a continuous predictor. High depth indicates diamonds are skinny and tall; low depth indicates diamonds are flat like a pancake.

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(col = depth), lwd = 1, alpha = 0.5) + scale_colour_gradientn(colours = rainbow(5))
```

It seems people like flatter diamonds and are willing to pay more per carat. Let's see this in the regression:

```{r}
mod = lm(price ~ carat + depth, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * depth, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

If carat increases by one unit, how much does price increase by? A tiny amount of increase.

How about cut?


```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(col = cut), lwd = 0.5) + scale_color_brewer(type = "div")
```

Likely something here.

```{r}
mod = lm(price ~ carat, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * cut, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

Yes.

Can we include all these interactions?

```{r}
mod = lm(price ~ carat + color + depth + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * (color + depth + cut), diamonds)
summary(mod)$r.squared
summary(mod)$sigma
coef(mod)
```

A decent gain once again.

What does the design matrix look like there? What is $p$?

```{r}
diamonds$cut = factor(diamonds$cut, ordered = FALSE)
Xmm = model.matrix(price ~ carat * (color + depth + cut), diamonds)
head(Xmm)
```


Can we take a look at interactions of two categorical variables?


```{r}
plot1 = ggplot(diamonds, aes(x = cut, y = color)) +
  geom_jitter(aes(col = price), lwd = 0.5) + scale_colour_gradientn(colours = rainbow(5))
plot1
```

Cool animation possible. May not work because it needs a ton of packages...

```{r}
pacman:::p_load_gh("dgrtwo/gganimate")
plot1 + transition_time(price)
```

Not so clear what's going on here. Let's see what the regressions say:


```{r}
mod = lm(price ~ color + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ color * cut, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

Not too much gain.



# More advanced modeling


```{r}
rm(list = ls())
pacman::p_load(ggplot2)
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
#drop some obviously nonsense observations
diamonds = diamonds[diamonds$carat <= 2 & diamonds$x != 0 & diamonds$y != 0 & diamonds$z != 0 & diamonds$depth != 0 & diamonds$table != 0,]
diamonds$ln_price = log(diamonds$price)
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
n = nrow(diamonds)
set.seed(1984)
diamonds = diamonds[sample(1 : n), ]
```


Note: I will now model price, not ln_price as ln_price yields residuals that are orders of magnitude. Fitting a good ln_price model will take more time.

```{r}
#All model formulas for reuse later
model_formulas = list(
  A = price ~ ln_carat,
  B = price ~ ln_carat * clarity,
  C = price ~ ln_carat * (clarity + cut + color),
  D = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut + color),
  E = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + carat + x + y + z + depth + table) * (clarity + cut + color)
)
#Model A
mod = lm(model_formulas[["A"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model B
mod = lm(model_formulas[["B"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model C
mod = lm(model_formulas[["C"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model D
mod = lm(model_formulas[["D"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model E 
mod = lm(model_formulas[["E"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
```

Big win on E... the reason I think is that now we have a flexible curve for each continuous feature and that curve changes with all the categorical features.

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!


```{r}
#Model F
model_formulas[["F"]] = price ~ 
        (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + poly(carat, 3) + poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * (cut + color + clarity)
mod = lm(model_formulas[["F"]], diamonds)
summary(mod)$sigma
mod$rank
```

Can you think of any other way to expand the candidate set curlyH? Discuss.

We now have a very flexible curve for each continuous feature and that curve is fit individually for each level of all categories. We could indeed go further by allowing the levels among cut, color and clarity to interact and further allowing the continous features to interact. 

There is another strategy to increase the complexity of H without using function transforms of the features and interactions... we will have to see after winter break...

We should probably assess oos performance now. Sample 4,000 diamonds and use these to create a training set of 3,600 random diamonds and a test set of 400 random diamonds. Define K and do this splitting:

```{r}
K = 10
n_sub = 4000
set.seed(1984)
n_test = 1 / K * n_sub
n_train = n_sub - n_test
test_indicies = sample(1 : n, n_test)
train_indicies = sample(setdiff(1 : n, test_indicies), n_train)
all_other_indicies = setdiff(1 : n, c(test_indicies, train_indicies))
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 3600.

```{r}
oos_se = list()
all_models_train = list()
for (model_idx in LETTERS[1 : 6]){
  all_models_train[[model_idx]] = lm(model_formulas[[model_idx]], diamonds[train_indicies, ])
  summary(all_models_train[[model_idx]])$sigma
  oos_se[[model_idx]] = sd(diamonds$price[test_indicies] - predict(all_models_train[[model_idx]], diamonds[test_indicies, ]))
}
oos_se
```

You computed oos metrics only on n_* = 400 diamonds. What problem(s) do you expect in these oos metrics?

They are variable. And something is wrong with F! Possibly Runge's phenomenon?

To do the K-fold cross validation we need to get the splits right and crossing is hard. I've found code for this on the web:

```{r}
set.seed(1984)
temp = rnorm(n_sub)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 100)
```

Do the K-fold cross validation for all models and compute the overall s_e and s_s_e. 

```{r}
oos_se = list()
oos_s_se = list()
for (model_idx in LETTERS[1 : 6]){
  e_vec_k = list() #for each one
  for (k in 1 : K){
    test_indicies_k = which(folds_vec == k)
    train_indicies_k = which(folds_vec != k)
    mod = lm(model_formulas[[model_idx]], diamonds[train_indicies_k, ])
    e_vec_k[[k]] = sd(diamonds$price[test_indicies_k] - predict(mod, diamonds[test_indicies_k, ]))
  }
  oos_se[[model_idx]] = mean(unlist(e_vec_k)) #note: not exactly the overall sd, but close enough
  oos_s_se[[model_idx]] = sd(unlist(e_vec_k))
}
res = rbind(unlist(oos_se), unlist(oos_s_se))
rownames(res) = c("avg", "sd")
res
```

Does K-fold CV help reduce variance in the oos s_e?

Yes because it is an average so its standard error is approximately s_e / sqrt(K). It's only approximate since this is the formula for the std err or independent samples and the K oos samples are not exactly independent.

Model F seems more variable than all of them. Why? Possibly Runge's phenomenon?

Imagine using the entire rest of the dataset besides the 4,000 training observations divvied up into slices of 400. Measure the oos error for each slice and also plot it.

```{r}
n_step = 1 / K * n_sub
oos_se = list()
ses = list()
starting_ks = seq(from = 1, to = (length(all_other_indicies) - n_step), by = n_step)
for (model_idx in LETTERS[1 : 6]){
  se_k = list() #for each one
  for (k in 1 : length(starting_ks)){
    diamonds_k = diamonds[all_other_indicies[starting_ks[k] : (starting_ks[k] + n_step - 1)], ]
    se_k[[k]] = sd(diamonds_k$price - predict(all_models_train[[model_idx]], diamonds_k))
  }
  ses[[model_idx]] = se_k
  oos_se[[model_idx]] = unlist(se_k)
}

pacman::p_load(reshape2)
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model")
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model") + ylim(0, 5000)
```

What do we learn from this? Model F is very risky. What exactly is going wrong? Part of the agony of data science is the "debugging" just like when debugging software. But here the errors can be in the modeling too! 

```{r}
max(unlist(ses[["F"]]))
k_BAD = which.max(unlist(ses[["F"]]))

diamonds_BAD = diamonds[all_other_indicies[starting_ks[k_BAD] : (starting_ks[k_BAD] + n_step - 1)], ]
diamonds_BAD

e_BAD = diamonds_BAD$price - predict(all_models_train[[model_idx]], diamonds_BAD)
tail(sort(abs(e_BAD)))
diamonds_BAD[which.max(abs(e_BAD)), ]

summary(diamonds[train_indicies, ])
```

Is it extrapolation? Yes... y = 58.9 and in a cubic function, that would be astronomical. The real mistake is that y = 58.9 is impossible. The data wasn't cleaned! 

**** Cleaning data essentially means visualizing each feature and the response (histograms for numeric and barplots for categorical) and ensuring there's no mistakes.

But this happens all the time and you don't want to be using polynomial functions for this reason since the new data may extrapolate very poorly.

Luckily we won't be dealing with these much longer since there are less risky ways of creating a transformed basis of Xraw.
