---
title: "Practice Lecture 6 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

```{r}
sample_mode = function(data){
  mode_val = names(sort(-table(data)))[1]
  switch(class(data),
    factor = factor(mode_val, levels = levels(data)),
    numeric = as.numeric(mode_val),
    integer = as.integer(mode_val),
    mode_val
  )
}
```

## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 10] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

First question. Let $\mathcal{H}$ be the set $\{0, 1\}$ meaning $g = 0$ or $g = 1$. What are the error rates then on $\mathbb{D}$? 

```{r}
#If always 0, all the 1's are errors
239 / (444 + 239)
#If always 1, all the 0's are errors
444 / (444 + 239)

g0 = function(){
  sample_mode(y_binary) #return mode regardless of x's
} 

g0()
```

If your $g$ can't beat that, either your features $x_1, \ldots, x_p$ are terrible, and/or $\mathcal{H}$ was a terrible choice and/or $\mathcal{A}$ can't pull its weight.

Okay... back to the "perceptron learning algorithm". Let's use the code from lab. I've added a warning if it didn't converge. And a verbose option to print out the iterations. And a casting to matrix.

```{r}
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w_0 = NULL, verbose = FALSE){
  p = ncol(Xinput)
  n = nrow(Xinput)
  Xinput = as.matrix(cbind(1, Xinput))
  if (is.null(w_0)){
    w_0 = rep(0, p + 1)
  }
  w_prev = w_0
  w = w_0
  for(iter in 1 : MAX_ITER){
    if (verbose & iter %% 10 == 0){
      cat("  perceptron iteration:", iter, "\n")
    }
    for (i in 1 : n) {
      x_i = Xinput[i,]
      y_hat_i = ifelse(sum(w_prev * x_i) >= 0, 1, 0)
      w = w + (y_binary[i] - y_hat_i) * x_i
    }
    if (identical(w, w_prev)){
      break
    }
    if (iter == MAX_ITER){
      warning("Perception did not converge in ", MAX_ITER, " iterations.")
    }
    w_prev = w
  }
  w
}
```

Let's do so for one dimension - just "V1" in the breast cancer data. You will do an example with more features for the lab.

```{r}
X1 = X[, 1, drop = FALSE]
w_vec = perceptron_learning_algorithm(X1, y_binary)
w_vec
```

It didn't converge. But it returned something... What is the error rate?

```{r}
yhat = ifelse(as.matrix(cbind(1, X1)) %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Looks like the perceptron fit to just the first feature beat the null model (at least on the data in $\mathbb{D}$). Is this expected? Yes if the first feature is at all predictive of `y`. Not bad considering it wasn't designed to even handle non-linearly separable datasets!

Let's try all features

```{r}
w_vec = perceptron_learning_algorithm(X, y_binary, verbose = TRUE)
w_vec
yhat = ifelse(as.matrix(cbind(1, X)) %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Still not bad. Even though the dataset is not linearly separable, the perceptron algorithm still "travels in the right direction".

Nobody uses perceptrons due to this linear separability problem. 

# Intro to Optimization in R

You will code a basic SVM for lab. In order to do so, you need to minimize the Vapnik Objective function. We use the `optimx` library. It has many different options for doing optimization including gradient descent (which we studied in 340). Optimization is really the subject of a whole other class which I suggest you all take.

```{r}
pacman::p_load(optimx)
```

Here's an example of locating the minimum of (w_1 - 3)^2+5 + (w_2 + 4)+12. The minimum is 17 and it occurs when w_1 = 3 and w_2 = -4.

```{r}
wvec_0 = c(0, 0) #the starting values
objective_function = function(wvec){(wvec[1] - 3)^2+5 + (wvec[2] + 4)^2+12}
#this optimization runs minimization by default
optimx(wvec_0, objective_function, 
       method = c('Nelder-Mead', 'BFGS', 'CG', 'L-BFGS-B', 'nlm', 'nlminb', 'spg', 'ucminf', 'newuoa', 'bobyqa', 'nmkb', 'hjkb', 'Rcgmin', 'Rvmmin'))
```

Optimization becomes more accurate if we supply the gradient vector and Hessian matrix. This is usually worth it if it's easy to compute but it's usually difficult to compute.

```{r}
objective_function_gradient = function(wvec){c(2 * wvec[1] - 6, 2 * wvec[2] + 8)}
objective_function_hessian = function(wvec){matrix(c(2, 0, 0, 2), nrow = 2)}
optimx(wvec_0, objective_function, gr = objective_function_gradient, hess = objective_function_hessian, method = c('Nelder-Mead', 'BFGS', 'CG', 'L-BFGS-B', 'nlm', 'nlminb', 'spg', 'ucminf', 'newuoa', 'bobyqa', 'nmkb', 'hjkb', 'Rcgmin', 'Rvmmin'))
```

In this case, the function was so ridiculously easy to minimize (being differentiable and of dimension of only 2) we only get improvements in far off decimal places if we give it the first and second derivatives.

For the purposes of the homework, you should use the method "BFGS". In real life, you have to play around and see what happens! No optimization method works well for all situations. Read the documentation and read up on these methods on wikipedia.


## Support Vector Machines (SVM)

Here we use the `e1071` library.

```{r}
pacman::p_load(e1071)
```

We make a simple dataset first.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

And let's plot the data:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Now we fit a linear SVM. As for the cost, which is related to lambda from class (although I'm not entirely sure how exactly it is related), we will leave the default which is 1.

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = 1, scale = FALSE)
```

The model object can be queried to find the "support vectors" i.e. the observations that lie on the wedge. Let's visualize them too.

```{r}
Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)
Xy_simple$is_support_vector = NULL #cleanup
simple_viz_obj
```


Now we calculate the weight vector. This is technical and not covered in the class yet (or maybe never):

```{r}
w_vec_simple_svm = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
w_vec_simple_svm
w_norm = sqrt(sum(w_vec_simple_svm^2))
w_norm
```

We can also plot it. We have to convert from Hesse Normal form back into point-intercept form. Note that $b$ is the first entry of the `w_vec_simple_svm` vector

```{r}
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

We can also plot the wedge by plotting the top line (where b is augmented by 1) and the bottom line (where b is diminished by 1).

```{r}
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```



To understand the hyperparameter, let's introduce another data point so the training data is no longer linearly separable.

```{r}
Xy_simple = rbind(Xy_simple, c(0, 3.2, 3.2))
```

and plot it:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Let's try SVM at different $\lambda$ values.

```{r}
#From documentation:
#"Parameters of SVM-models usually must be tuned to yield sensible results!"

#lambda = 0.01
# lambda = 0.1
# lambda = 1
lambda = 2

Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
summary(svm_model)
w_vec_simple_svm = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
sqrt(sum(w_vec_simple_svm^2))

Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)

simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```

What lesson did we learn here? This hyperparameter really matters! We need to figure out a way to deal with selecting the "right" hyperparameter value automatically. So far neither the perceptron nor the SVM is an algorithm for binary classification that comes without flaws.

What are the support vectors now? Any data point that would change the dividing line if removed. It's a more expansive definition than when there was no hinge loss term.

Let's see how the SVM does on the breast cancer data.

```{r}
rm(list = ls())
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = as.matrix(Xy[, 2 : 10]) #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")

svm_model = svm(X, y_binary, kernel = "linear", cost = 3e-5)
w_vec = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% X[svm_model$index, ] # the other terms
)
yhat = ifelse(cbind(1, X) %*% w_vec > 0, 1, 0)
table(yhat)
sum(y_binary != yhat) / length(y_binary)
```

That did pretty well (after I tried a bunch of cost values for the hyperparameter).

## Nearest Neighbor (NN) model

Load up the breast cancer data set again.

```{r}
rm(list = ls())
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = as.matrix(Xy[, 2 : 10]) #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
```

Let's say we want to build a nearest neighbor model with the first covariate only. We are then looking for the label (response) of the closest x_1. Here is a simple function that does it:

```{r}
nn_function = function(x_star){
  y_binary[which.min((X[, 1] - x_star)^2)]
}
nn_function(7.8)
nn_function(5.2)
```

Why is this silly for this dataset?

```{r}
table(X[, 1])
```

The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
?knn
```

We can fit a NN model *and* predict in one shot via:

```{r}
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1, 1), y_binary, k = 1)
y_hat
```

Why is computing the model g and predicting in tandem natural in knn?

Now for an interesting exercise that will setup future classes:

```{r}
y_hat = knn(X, X, y_binary, k = 1)
y_hat
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No... "something" must be wrong! It is too good to be true.

Something is wrong. This is the first example of "overfitting". We will explore this later in depth (it is one of the core concepts of this course).

Let's see $K > 1$


```{r}
y_hat = knn(X, X, y_binary, k = 10)
all.equal(y_hat, factor(y_binary))
mean(y_hat != factor(y_binary))
```

Why would there be difference now between predictions and the actual data?

```{r}
rm(list = ls())
```



