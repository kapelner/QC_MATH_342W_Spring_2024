---
title: "Practice Lecture 5 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
rm(list = setdiff(ls(), "sample_mode"))
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 10] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

First question. Let $\mathcal{H}$ be the set $\{0, 1\}$ meaning $g = 0$ or $g = 1$. What are the error rates then on $\mathbb{D}$? 

```{r}
#If always 0, all the 1's are errors
239 / (444 + 239)
#If always 1, all the 0's are errors
444 / (444 + 239)

g0 = function(){
  sample_mode(y_binary) #return mode regardless of x's
} 

g0()
```

If your $g$ can't beat that, either your features $x_1, \ldots, x_p$ are terrible, and/or $\mathcal{H}$ was a terrible choice and/or $\mathcal{A}$ can't pull its weight.

Okay... back to the "perceptron learning algorithm". Let's use the code from lab. I've added a warning if it didn't converge. And a verbose option to print out the iterations. And a casting to matrix.

```{r}
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w_0 = NULL, verbose = FALSE){
  p = ncol(Xinput)
  n = nrow(Xinput)
  Xinput = as.matrix(cbind(1, Xinput))
  if (is.null(w_0)){
    w_0 = rep(0, p + 1)
  }
  w_prev = w_0
  w = w_0
  for(iter in 1 : MAX_ITER){
    if (verbose & iter %% 10 == 0){
      cat("  perceptron iteration:", iter, "\n")
    }
    for (i in 1 : n) {
      x_i = Xinput[i,]
      y_hat_i = ifelse(sum(w_prev * x_i) >= 0, 1, 0)
      w = w + (y_binary[i] - y_hat_i) * x_i
    }
    if (identical(w, w_prev)){
      break
    }
    if (iter == MAX_ITER){
      warning("Perception did not converge in ", MAX_ITER, " iterations.")
    }
    w_prev = w
  }
  w
}
```

Let's do so for one dimension - just "V1" in the breast cancer data. You will do an example with more features for the lab.

```{r}
X1 = X[, 1, drop = FALSE]
w_vec = perceptron_learning_algorithm(X1, y_binary)
w_vec
```

It didn't converge. But it returned something... What is the error rate?

```{r}
yhat = ifelse(as.matrix(cbind(1, X1)) %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Looks like the perceptron fit to just the first feature beat the null model (at least on the data in $\mathbb{D}$). Is this expected? Yes if the first feature is at all predictive of `y`. Not bad considering it wasn't designed to even handle non-linearly separable datasets!

Let's try all features

```{r}
w_vec = perceptron_learning_algorithm(X, y_binary, verbose = TRUE)
w_vec
yhat = ifelse(as.matrix(cbind(1, X)) %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Still not bad. Even though the dataset is not linearly separable, the perceptron algorithm still "travels in the right direction".


## Support Vector Machines (SVM)

You will code a basic SVM for homework. Here we use the `e1071` library.

```{r}
pacman::p_load(e1071)
```

We make a simple dataset first.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

And let's plot the data:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Now we fit a linear SVM. As for the cost, which is related to lambda from class (although I'm not entirely sure how exactly it is related), we will leave the default which is 1.

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = 1, scale = FALSE)
```

The model object can be queried to find the "support vectors" i.e. the observations that lie on the wedge. Let's visualize them too.

```{r}
Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)
Xy_simple$is_support_vector = NULL #cleanup
simple_viz_obj
```


Now we calculate the weight vector. This is technical and not covered in the class yet (or maybe never):

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
w_vec_simple_svm
w_norm = sqrt(sum(w_vec_simple_svm^2))
w_norm
```

We can also plot it. We have to convert from Hesse Normal form back into point-intercept form. Note that $b$ is the first entry of the `w_vec_simple_svm` vector

```{r}
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

We can also plot the wedge by plotting the top line (where b is augmented by 1) and the bottom line (where b is diminished by 1).

```{r}
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```



To understand the hyperparameter, let's introduce another data point so the training data is no longer linearly separable.

```{r}
Xy_simple = rbind(Xy_simple, c(0, 3.2, 3.2))
```

and plot it:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Let's try SVM at different $\lambda$ values.

```{r}
#From documentation:
#"Parameters of SVM-models usually must be tuned to yield sensible results!"

lambda = 0.01
# lambda = 0.1
# lambda = 1
# lambda = 2

Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
summary(svm_model)
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
sqrt(sum(w_vec_simple_svm^2))

Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)

simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```

What lesson did we learn here? This hyperparameter really matters! We need to figure out a way to deal with selecting the "right" hyperparameter value automatically. So far neither the perceptron nor the SVM is an algorithm for binary classification that comes without flaws.

What are the support vectors now? Any data point that would change the dividing line if removed. It's a more expansive definition than when there was no hinge loss term.


## Nearest Neighbor algorithm

Load up the breast cancer data set again.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = Xy[, 2 : 10] #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
```

Let's say we want to build a nearest neighbor model with the first covariate only. We are then looking for the label (response) of the closest x_1. Here is a simple function that does it:

```{r}
nn_function = function(x_star){
  y_binary[which.min((X[, 1] - x_star)^2)]
}
nn_function(7.8)
nn_function(5.2)
```

Why is this silly for this dataset?

```{r}
str(X)
```

The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
?knn
```

We can fit a knn model *and* predict in one shot via:

```{r}
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1, 1), y_binary, k = 1)
y_hat
```

Why is computing the model g and predicting in tandem natural in knn?

Now for an interesting exercise that will setup future classes:

```{r}
y_hat = knn(X, X, y_binary, k = 1)
y_hat
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No... "something" must be wrong! It is too good to be true.

Something is wrong. This is the first example of "overfitting". We will explore this later in depth (it is one of the core concepts of this course).

Let's see $K > 1$


```{r}
y_hat = knn(X, X, y_binary, k = 10)
y_hat
all.equal(y_hat, factor(y_binary))
```

Why would there be difference now between predictions and the actual data?

```{r}
rm(list = ls())
```



## Simple Linear Regression (p = 1)

To understand what the algorithm is doing - best linear fit by minimizing the squared errors, we can draw a picture. First let's make up some very simple training data $\mathbb{D}$.

```{r}
set.seed(1984)
n = 20
x = runif(n)
beta_0 = 3
beta_1 = -2
h_star = beta_0 + beta_1 * x
epsilons = rnorm(n, mean = 0, sd = 0.33)
y = h_star + epsilons 
```

And let's plot the data:


```{r}
pacman::p_load(ggplot2)
simple_df = data.frame(x = x, y = y)
simple_viz_obj = ggplot(simple_df, aes(x, y)) + 
  geom_point(size = 2)
simple_viz_obj
```

And its true $h^*$ line:

```{r}
true_hstar_line = geom_abline(intercept = beta_0, slope = beta_1, color = "green")
simple_viz_obj + true_hstar_line
```

Now let's calculate the simple least squares coefficients:

```{r}
r = cor(x, y)
s_x = sd(x)
s_y = sd(y)
ybar = mean(y)
xbar = mean(x)

b_1 = r * s_y / s_x
b_0 = ybar - b_1 * xbar
b_0
b_1
```

Note how $b_0$ and $b_1$ are not exactly the same as $\beta_0$ and $\beta_1$. Why?

And we can plot it:


```{r}
simple_ls_regression_line = geom_abline(intercept = b_0, slope = b_1, color = "red")
simple_viz_obj + simple_ls_regression_line + true_hstar_line
```

Review of the modeling framework:

The difference between the green line and red line is the "estimation error". The difference between the green line and the points is a combination of error due to ignorance and error due to misspecification of $f$ as a straight line. In most real-world applications, estimation error is usually small relative to the other two. In the era of "big data", $n$ is usually big so estimation error is pretty small.

Recall that the noise (epsilons) are the difference between the data and the green line:

```{r}
simple_df$hstar = beta_0 + beta_1 * simple_df$x
simple_viz_obj = ggplot(simple_df, aes(x, y)) + 
  geom_point(size = 2)
epsilon_line_segments = geom_segment(aes(xend = x, yend = hstar), position = position_nudge(x = 0.002))
simple_viz_obj + epsilon_line_segments + true_hstar_line
```

And that the residuals (e's) are the difference between the measurements of the response in the actual data and the green line:

```{r}
simple_df$gs = b_0 + b_1 * simple_df$x
simple_viz_obj = ggplot(simple_df, aes(x, y)) + 
  geom_point(size = 2)
e_line_segments = geom_segment(aes(xend = x, yend = gs), color = "purple")
simple_viz_obj + simple_ls_regression_line + e_line_segments
```

Examining both at the same time:

```{r}
simple_viz_obj + simple_ls_regression_line + true_hstar_line + e_line_segments + epsilon_line_segments
```

