---
title: "Practice Lecture 20 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

# Bias-Variance Decomposition of Generalization Error

Let's try to fit a quadratic $f$ with a linear model and examine bias-variance tradeoff.

```{r}
rm(list = ls())
xmin = 0
xmax = 5
n_train = 20
n_test = 1000
sigma = 1 # sigma-squared is the irreducible error component through all of the bias-variance decomp formulas
f = function(x){x^2}

#the number of datasets generated (separate universes) and thus the number of models fitted in different universes
Nsim = 1000

training_gs = matrix(NA, nrow = Nsim, ncol = 2)
x_trains = matrix(NA, nrow = Nsim, ncol = n_train)
y_trains = matrix(NA, nrow = Nsim, ncol = n_train)
all_oos_residuals = matrix(NA, nrow = Nsim, ncol = n_test)
for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sigma) #Assumption I: mean zero and Assumption II: homoskedastic
  y_train = f(x_train) + delta_train
  x_trains[nsim, ] = x_train
  y_trains[nsim, ] = y_train
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ ., data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset according to the same data generating process (DGP) 
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigma)
  y_test = f(x_test) + delta_test
  #predict oos using the model and save the oos residuals
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_oos_residuals[nsim, ] = y_test - y_hat_test
}
```

Take a look at the irreducible error for one dataset:

```{r}
pacman::p_load(ggplot2)
resolution = 10000 #how many xstars to test
x = seq(xmin, xmax, length.out = resolution)

f_x_df = data.frame(x = x, f = f(x))
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "green") + 
  geom_point(aes(x, y), data = data.frame(x = x_trains[1, ], y = y_trains[1, ]))
```

There is no way to fit those deviations from the green line using known information. That is the "irreducible error" (from ignorance).

There is dataset to dataset variation (different universes) in these deltas. Graph a few:

```{r}
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "green") +
  geom_point(aes(x, y), data = data.frame(x = x_trains[1, ], y = y_trains[1, ]), col = "blue") +
  geom_point(aes(x, y), data = data.frame(x = x_trains[2, ], y = y_trains[2, ]), col = "darkgreen") +
  geom_point(aes(x, y), data = data.frame(x = x_trains[3, ], y = y_trains[3, ]), col = "red")
```

The blue dataset is one possible $\mathbb{D}$, the green dataset is another possible $\mathbb{D}$ and the red dataset is another possible $\mathbb{D}$. This illustrated "dataset-dataset variability".

Each of the datasets yields slightly different linear models:

```{r}
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "green") + 
  geom_abline(intercept = training_gs[1, 1], slope = training_gs[1, 2], col = "blue") +
  geom_abline(intercept = training_gs[2, 1], slope = training_gs[2, 2], col = "darkgreen") +
  geom_abline(intercept = training_gs[3, 1], slope = training_gs[3, 2], col = "red") +
  ylim(-3, 25)
```



Let's visualize the bias, the distance between the green line f(x) and the average model, E[g(x)]:

```{r}
g_average = colMeans(training_gs)
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "green") + 
  geom_abline(intercept = g_average[1], slope = g_average[2], color = "red") +
  ylim(-3, 25)
```

What is the average bias of $g$? That is, the average distance between E[g(x)] and f(x) where the average is taken over the distribution of x, which was uniform between xmin and xmax:

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x
biases = f(x) - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

What is the variance? Let's look at 100 lines this time instead of three:

```{r}
plot_obj = ggplot() + 
  xlim(xmin, xmax) + ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 100)){ #otherwise takes too long
  plot_obj = plot_obj + geom_abline(intercept = training_gs[nsim, 1], slope = training_gs[nsim, 2], col = "blue")
}

plot_obj +
  geom_abline(intercept = g_average[1], slope = g_average[2], col = "red", lwd = 2) 
  # geom_line(data = f_x_df, aes(x, f), col = "green", size = 1)
```

Now calculate this average variance:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Take a look at the mse that's averaging over (1) all datasets D and (2) all xstars in the set [0,5]. This should be equal to the three terms: (a) irreducible error plus (b) bias-squared plus (c) variance.

```{r}
mse = mean(c(all_oos_residuals)^2)
mse
```

Now check the equivalence

```{r}
sigma^2
expe_bias_g_sq
expe_var_g
sigma^2 + expe_bias_g_sq + expe_var_g
```

This is not exactly equal due to numerical error. If we increase resolution and Nsim, it will be equal.

What's the conclusion: the linear model is a high bias / low variance model.

Let's try the whole thing again using a quadratic regression!

```{r}
training_gs = matrix(NA, nrow = Nsim, ncol = 3)
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sigma)
  y_train = f(x_train) + delta_train
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ poly(x, 2, raw = TRUE), data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigma)
  y_test = f(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Much lower! Why? Bias went down. 

Let's visualize the bias

```{r}
g_average = colMeans(training_gs)
x = seq(xmin, xmax, length.out = resolution)
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "green") + 
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2}, col = "red")
```

Not much! What is the average bias of $g$?

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x + g_average[3] * x^2
f_x = x^2
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

What is the variance? Let's look at all lines:

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) + ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2), mapping = aes(x, y), col = "blue")
}

plot_obj +
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2}, col = "red", lwd = 2) #+
  #geom_line(data = f_x_df, aes(x, f), col = "green", size = 0.5) 
```

Not so much variance.

Now calculate this average variance:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x + g_average[3] * x^2

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Now check the equivalence

```{r}
mse
sigma^2 
expe_bias_g_sq
expe_var_g
sigma^2 + expe_bias_g_sq + expe_var_g
```


Try it again with quintic polynomials!

```{r}
training_gs = matrix(NA, nrow = Nsim, ncol = 6)
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sigma)
  y_train = f(x_train) + delta_train
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ poly(x, 5, raw = TRUE), data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigma)
  y_test = f(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Much higher! Why? 

Variance went up!

Let's visualize the bias

```{r}
g_average = colMeans(training_gs)
f = function(x){x^2}
x = seq(xmin, xmax, length.out = resolution)
ggplot(f_x_df, aes(x, f)) + 
  geom_line(col = "darkgreen") + 
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5}, col = "red")
```

Not much! Now compute the average bias squared of $g$:

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5
biases = f(x) - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

This appears to have increased over last time by a nominal amount ... but it's only because we're not running the regression infinite times. Remember this "expectation" is only an average.

What is the variance? Let's look at all lines:

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax)

for (nsim in 1 : min(Nsim, 30)){ #otherwise takes too long
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2 + training_gs[nsim, 4] * x^3 + training_gs[nsim, 5] * x^4 + training_gs[nsim, 6] * x^5), mapping = aes(x, y), col = "blue")
}

plot_obj +
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5}, col = "red", lwd = 2) + 
  ylim(0, 25) +
  geom_line(data = f_x_df, aes(x, f), col = "green", size = 0.5)
```

Some of them looks awful! Welcome to Runge's and overfitting.

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2 + training_gs[nsim, 4] * x^3 + training_gs[nsim, 5] * x^4 + training_gs[nsim, 6] * x^5
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Conclusion: any more complexity than you need allows for variance which comes from overfitting! Overfitted models induce variance-error from the perspective of multiple universes.

Now check the equivalence:

```{r}
mse
sigma^2
expe_bias_g_sq
expe_var_g
sigma^2 + expe_bias_g_sq + expe_var_g
```

# Bias - Variance Decomposition of MSE in Regression Trees

Let's return to the simulated sine curve data which we used to introduce regression trees.

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigma = 0.2
Nsim = 250
```

And load the tree package:

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF, tidyverse, magrittr)
```

Now let's generate lots of different datasets and fit many regression tree models.

```{r}
training_gs = list() #storing entire objects - need a hash
Nsim = 200 #trees take longer to build than OLS models
sigma = 1
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

#go back to the sine curve example from class when we introduced the tree model
f = function(x){5 * sin(x)}

for (nsim in 1 : Nsim){
  cat ("nsim", nsim, "of", Nsim, "\n")
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sigma)
  y_train = f(x_train) + delta_train
  # ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
  
  #fit a model g | x's, delta's and save it
  g_model = YARFCART(data.frame(x = x_train), y_train, calculate_oob_error = FALSE, verbose = FALSE)
  training_gs[[nsim]] = g_model
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigma) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = f(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Let's look at the last of the data sets to remind ourselves of the problem setting:

```{r}
ggplot(data.frame(x = x_train, y = y_train)) + 
  geom_point(aes(x, y))
```


What does the storage of all the models look like?

```{r}
head(training_gs, 2)
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Let's visualize the bias

```{r}
resolution = 1000
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)

#now estimate the expectation of g by averaging all the different models
g_avg_x = array(0, resolution)
for (nsim in 1 : Nsim){
  g_nsim = training_gs[[nsim]]
  g_avg_x = g_avg_x + predict(g_nsim, data.frame(x = x))
}
g_avg_x = g_avg_x / Nsim #average of all models

#now plot
ggplot(data.frame(x = x, f = f(x), expe_g = g_avg_x)) + 
  geom_line(aes(x, expe_g), col = "red", lwd = 2) +
  geom_line(aes(x, f), col = "green", lwd = 1)
```

Not much! Now actually compute the average bias squared of $g$:

```{r}
biases = f(x) - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

This is small - why??

It's because trees are so expressive and have such model complexity that they can nail almost any true $f$ function! The default tree at N_0 = 5 overfits. That overfitting gives us very little bias.

That means the MSE save the irreducible noise is coming from the variance. Let's look at the variance vs f(x):

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) #+ ylim(xmin^2, xmax^2)

num_trees_to_visualize = 30
for (nsim in 1 : min(Nsim, num_trees_to_visualize)){
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = g_x), aes(x = x, y = y), col = "blue")
}

plot_obj + 
  geom_line(data = data.frame(x = x, expe_g = g_avg_x), mapping = aes(x, expe_g), col = "red", lwd = 2) +
  geom_line(aes(x, f(x)), col = "green", lwd = 1)
```

It looks awful!!! Why? Because they overfit.

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

var_x_s = array(NA, min(Nsim, 50))
for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  var_x_s[nsim] = mean((g_x - g_avg_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Any more complexity than you need allows for overfitting!

Now check the equivalence

```{r}
mse
sigma^2
expe_bias_g_sq
expe_var_g
sigma^2 + expe_bias_g_sq + expe_var_g
```


# Model Averaging via Bootstrap Aggregation ("Bagging")

Now let's generate one dataset (only ONE)!

```{r}
x_train = runif(n_train, xmin, xmax)
delta_train = rnorm(n_train, 0, sigma)
y_train = f(x_train) + delta_train
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```

Spend a moment to appreciate that this is all we're going to have. There is a lot of irreducible noise that is going to prevent us from finding the sine curve.

Now we create 250 bootstrap samples to create similiar but different datasets and fit 250 tree models:

```{r}
bootstrapped_gs = list() #storing entire objects - need a hash / list object

num_trees = 250 #M = num_trees
for (t in 1 : num_trees){ 
  #fit a model g | x's, delta's and save it
  bootstrap_indices_t = sample(1 : n_train, replace = TRUE)
  g_model = YARFCART(data.frame(x = x_train[bootstrap_indices_t]), y_train[bootstrap_indices_t], 
                     calculate_oob_error = FALSE) #this is confusing! But will become clear later...
  bootstrapped_gs[[t]] = g_model
}
```

Now let's aggregate all models constructed with bootstrap samples together by averaging and see what it looks like relative to real $f$:

```{r}
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)

#create the bagged model predictions
g_bagged = array(0, resolution)
for (t in 1 : num_trees){
  g_t = bootstrapped_gs[[t]]
  g_bagged = g_bagged + predict(g_t, data.frame(x = x))
}
g_bagged = g_bagged / num_trees #average of all models

#now plot
ggplot(data.frame(x = x, f = f(x), g_bagged = g_bagged)) + 
  geom_line(aes(x, f), col = "green") + 
  geom_line(aes(x, g_bagged), col = "blue")
```
  
That's pretty good considering the plot of the raw data from above.

We just compared one tree to a bagged-tree (i.e. average of many trees where each tree was fit on a bootstrap sample of the original data). Let's see if the bagged tree is truly better than one tree. The way to do this is generate lots of datasets and try to estimate mse (with bias and variance).

We fit the bag of trees before manually with a for loop. But of course there is a convenience to do this, the function `YARFBAG`. And then we run the same simulation as we did above with one tree.

```{r}
Nsim = 250
training_gs = list() #storing entire objects - need a hash
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sigma)
  y_train = f(x_train) + delta_train
  # ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
  
  #fit a model bagged tree g | x's, delta's and save it
  g_model = YARFBAG(data.frame(x = x_train), y_train, num_trees = num_trees, calculate_oob_error = FALSE) #automatically does the bootstrap internally
  training_gs[[nsim]] = g_model
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigma) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = f(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

What do all the the bagged models look like?

```{r}
head(training_gs, 2)
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

This is much lower than before.

Let's visualize the bias:

```{r}
resolution = 1000
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)
f_x = sin(x)

#now estimate the expectation of g by averaging all the different models
g_avg_x = array(0, resolution)
for (nsim in 1 : Nsim){
  g_nsim = training_gs[[nsim]]
  g_avg_x = g_avg_x + predict(g_nsim, data.frame(x = x))
}
g_avg_x = g_avg_x / Nsim #average of all models

#now plot
ggplot(data.frame(x = x, f = f(x), expe_g = g_avg_x)) + 
  geom_line(aes(x, expe_g), col = "red", lwd = 2) +
  geom_line(aes(x, f), col = "darkgreen", lwd = 1) 
```

This is not a shock since if a single tree was unbiased then an average of trees will be unbiased all the more so.

Now actually compute the average bias squared of $g$:

```{r}
biases = f(x) - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

Almost nothing.

Now... variance? Here's where the juice to bagging is...

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) #+ ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 30)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = g_x), aes(x = x, y = y), col = "blue")
}

plot_obj +
  geom_line(data = data.frame(x = x, expe_g = g_avg_x), mapping = aes(x, expe_g), col = "red", lwd = 1.5)
```

There is variance, but less than previously when only one tree was used.

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

var_x_s = array(NA, min(Nsim, 50))
for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  var_x_s[nsim] = mean((g_x - g_avg_x)^2)
}

expe_var_g_bagged = mean(var_x_s)
expe_var_g_bagged
```

Any more complexity than you need allows for overfitting!

Now check the equivalence

```{r}
mse
sigma^2
expe_bias_g_sq
expe_var_g_bagged
sigma^2 + expe_bias_g_sq + expe_var_g_bagged
```

We have a better algorithm! Why? Because there is reduction of correlation between the bootstrapped models:

```{r}
rho = expe_var_g_bagged / expe_var_g
rho
```

That's a gain of ~50% in the MSE component that we have control over! And... it's for FREE; we didn't need additional data! 

Well, not exactly, we are giving up something... the single tree's interpretability - the ultimate tradeoff in machine learning. This tradeoff is no joke! It is a huge area of research and a huge debate of civil importance. Let's watch some of Stephen Wolfram's testimony in the US Senate in Feb, 2021.

https://www.youtube.com/watch?v=SuaP8izUPzQ&t=424s
https://www.youtube.com/watch?v=SuaP8izUPzQ&t=588s

# Bagged Trees vs. a Linear Model: Behold the power of machine learning!!

Let's look at the boston housing data first.

```{r}
rm(list = ls())
boston = MASS::Boston

prop_test = 0.1
test_indices = sample(1 : nrow(boston), round(prop_test * nrow(boston)))
boston_test = boston[test_indices, ]
y_test = boston_test$medv
X_test = boston_test
X_test$medv = NULL
train_indices = setdiff(1 : nrow(boston), test_indices)
boston_train = boston[train_indices, ]
y_train = boston_train$medv
X_train = boston_train
X_train$medv = NULL

mod_lin = lm(y_train ~ ., X_train)
y_hat_test_lin = predict(mod_lin, X_test)
oos_rmse_lin = sd(y_test - y_hat_test_lin)
oos_rmse_lin

num_trees = 500
mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
y_hat_test_bag = predict(mod_bag, X_test)
oos_rmse_bag = sd(y_test - y_hat_test_bag)
oos_rmse_bag

cat("oos standard error reduction:", (1 - oos_rmse_bag / oos_rmse_lin) * 100, "%\n")
```


Whole lot of room to improve! That RMSE improvement comes from effectively removing the bulk of the linear model's bias while still keeping variance reasonable (as opposed to crazy polynomial models and tons of interactions).

Now the diamonds data:

```{r}
n_train = 1000

training_indices = sample(1 : nrow(diamonds), n_train)
diamonds_train = diamonds[training_indices, ]
y_train = diamonds_train$price
X_train = diamonds_train
X_train$price = NULL


test_indices = setdiff(1 : nrow(diamonds), training_indices)
diamonds_test = diamonds[test_indices, ]
y_test = diamonds_test$price
X_test = diamonds_test
X_test$price = NULL


mod_lin = lm(y_train ~ ., X_train)
y_hat_test_lin = predict(mod_lin, X_test)
oos_rmse_lin = sd(y_test - y_hat_test_lin)
oos_rmse_lin

mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
y_hat_test_bag = predict(mod_bag, X_test)
oos_rmse_bag = sd(y_test - y_hat_test_bag)
oos_rmse_bag

cat("oos standard error reduction:", (1 - oos_rmse_bag / oos_rmse_lin) * 100, "%\n")
```

That's a pretty bg improvement... and it's even more shocking that...

```{r}
summary(mod_lin)$r.squared
```

There was seemingly not a whole lot of room to improve! But there is when you measure error in RMSE. That last 7% of unexplained variance means (1) a whole lot of RMSE and (2) that whole lot of RMSE is split between bias and irreducible error due to unknown information. But it did the best it can. It is likely what's remaining is due to information we don't measure.

# Validation in Bagging?

We are using the "bootstrap" to get the trees. Can we do model validation in the same step? 

The answer is yes. For every tree, there was a bootstrap sample of the training set used to build the tree. But there are observations in $\mathbb{D}$ that are not in the bootstrap sample! About 1/3 on average are left out i.e. "out of bag (oob)". Over many trees, there are different oob subsets than become the full data set. So you actually have validation in a way on the whole dataset kind of like K-fold cross validation. Supposedly this validation is similar to K=2 in terms of performance. It is what everyone seems to use. 

Let's load the data and packages from last class and plot the data:

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigma = 0.09
num_trees = 500
x_train = runif(n_train, xmin, xmax)
delta_train = rnorm(n_train, 0, sigma)
y_train = sin(x_train) + delta_train
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```

Let's look at one bagged tree model and compute OOB errors after construction. Here we just drop the `calculate_oob_error` argument as the default is `TRUE`. We save the model and print it out:

```{r}
bagged_tree_mod = YARFBAG(data.frame(x = x_train), y_train, num_trees = num_trees)
bagged_tree_mod
```

How did this work? Let's look at the oob sets of indices:

```{r}
cat("bootstrap indices:")
sort(bagged_tree_mod$bootstrap_indices[[1]])
cat("oob:")
sort(setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[1]]))
cat("bootstrap indices:")
sort(bagged_tree_mod$bootstrap_indices[[2]])
cat("oob:")
sort(setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[2]]))
bagged_tree_mod$y_oob

n_oob = sapply(1 : n_train, function(i){sum(unlist(lapply(bagged_tree_mod$bootstrap_indices, function(set){!(i %in% set)})))})
round(n_oob / num_trees, 2)
mean(n_oob / num_trees, 2)
1 / exp(1)
```

It took predictions on each tree on the oob set, averaged by observation across trees and then averaged across observation averages.

Let's compare this to training-test manual splits. Let's look at the boston housing data first.

```{r}
pacman::p_load(data.table)
boston = MASS::Boston %>% data.table

seed = 4
set.seed(seed)
prop_test = 0.5
test_indices = sample(1 : nrow(boston), round(prop_test * nrow(boston)))
boston_test = boston[test_indices, ]
y_test = boston_test$medv
X_test = boston_test
X_test$medv = NULL
train_indices = setdiff(1 : nrow(boston), test_indices)
boston_train = boston[train_indices, ]
y_train = boston_train$medv
X_train = boston_train
X_train$medv = NULL

num_trees = 500

#build on training and validate on test
mod_bag_train = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE, seed = seed)
y_hat_test_bag = predict(mod_bag_train, X_test)
s_e_bag = sd(y_test - y_hat_test_bag)
s_e_bag

#build and validate on all the data at once!
mod_bag_all = YARFBAG(boston[, !"medv"], boston$medv, num_trees = num_trees, seed = seed)
mod_bag_all$rmse_oob
```

There is a lot of variation, but theoretically, they should be about the same. 

What do we have now? We now have model selection done within training. And training and validation are done in a single step! No more costly K-fold CV with 3 splits!


# Random Forests

Can we do any better? YES. As you saw, the variance terms can be shrunk further the more decorrelated the trees become. We do this now by introducing randomness into the splits by choosing only a subset of the features to split on randomly. The trees are then grown as normal. Then the we model average many trees via bagging. And that's random forests!

Quick demo with the diamonds and oob validation:

```{r}
rm(list = ls())

seed = 1984
set.seed(seed)
n_samp = 2000
#note: trees are perfect for ordinal variables! Since it only needs an ordering to do the splits
#and thus it is immune to different encodings so, just use the standard 1, 2, 3, ...
#thus there is no need here to cast the ordinal variables cut, color, clarity to nominal variables
diamonds_samp = diamonds %>% sample_n(n_samp)
y = diamonds_samp$price
X = diamonds_samp %>% dplyr::select(-price)

num_trees = 1000
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
mod_bag

#now for apples-apples, build the RF on the same bootstrap data for each tree
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices)
mod_rf

# pacman::p_load(randomForest)
# mod_rf = randomForest(X, y, num_trees = num_trees)
# mod_rf

cat("gain: ", (mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, "%\n")
```

This was a fail. RF is sensitive to the number of features samples (the hyperparameter mtry). Let's try again.


```{r}
#there are 9 total features
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices, mtry = 8)
mod_rf

cat("gain: ", (mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, "%\n")
```

This is not so impressive. But it is for free. 

If `mtry` is small, the gain may be so small in the rho-multiple on the variance term that it doesn't outweigh the increase in bias. Thus, we underfit a little bit. Thus it's better to stay with just bagging. Here it is on the boston housing data:

```{r}
rm(list = ls())
y = MASS::Boston$medv
X = MASS::Boston
X$medv = NULL
seed = 1984
num_trees = 1000
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
mod_bag
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices, mtry = 10)
mod_rf
cat("oob rmse gain:", round((mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, 3), "%\n")
```

Slightly more impressive. But again it is for free. 

What about for classification models?

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult %<>% 
  na.omit #kill any observations with missingness
n_samp = 1000
seed = 1984
set.seed(seed)
adult_samp = adult %>% sample_n(n_samp)
y = adult_samp$income
X = adult_samp %>% dplyr::select(-income)

num_trees = 500
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
#default mtry for classification models is floor(sqrt(p))
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices) 
mod_bag$misclassification_error
mod_rf$misclassification_error

cat("gain: ", (mod_bag$misclassification_error - mod_rf$misclassification_error) / mod_bag$misclassification_error * 100, "%\n")
```

Very nice... and can likely be improved with optimizing mtry.

And on letters:

```{r}
rm(list = ls())
pacman::p_load(mlbench, skimr)
data(LetterRecognition)
LetterRecognition = na.omit(LetterRecognition) #kill any observations with missingness
#skim(LetterRecognition)
?LetterRecognition

adult %<>% 
  na.omit #kill any observations with missingness
n_samp = 2000
seed = 1984
set.seed(seed)
letters_samp = LetterRecognition %>% sample_n(n_samp)
y = letters_samp$lettr
X = letters_samp %>% dplyr::select(-lettr)

num_trees = 500
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
#default mtry for classification models is floor(sqrt(p))
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices) 
mod_bag$misclassification_error
mod_rf$misclassification_error

cat("gain: ", (mod_bag$misclassification_error - mod_rf$misclassification_error) / mod_bag$misclassification_error * 100, "%\n")
```

Very nice... and can likely be improved with optimizing mtry.

There are very real gains for RF over bagging and these are most pronounced in classification models.


