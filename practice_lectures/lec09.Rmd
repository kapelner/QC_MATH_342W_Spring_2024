---
title: "Practice Lecture 9 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

## OLS using categorical predictors

Note that historically this is called "Analysis of Variance" or "ANOVA" for short. But there is no difference to the computer, it still crunches the same matrices.

Let's get the cars data again:

```{r}
cars = MASS::Cars93
str(cars)
```

Let's try to model `Type`, a factor with 6 levels.

```{r}
table(cars$Type)
```

What will $\hay{y}$ look like? Should be the $\bar{y}$'s for each level. What is $p$? 6. Let' see:

```{r}
anova_mod = lm(Price ~ Type, cars)
coef(anova_mod)
summary(anova_mod)$r.squared
```

The one categorical variable got blown up into 5 features. How to interpret? First need to know the "reference category" i.e. which level is missing in the list. We can see from cross-referencing the coefficient names with the table of the raw feature that the reference category is `Compact`. So what is prediction for the compact type? The intercept. What is prediction of Large type? Intercept + Large, etc. 

What actually happened to get the OLS estimates? Let's see the model matrix:

```{r}
Xmm = model.matrix(Price ~ Type, cars)
head(Xmm, 20)
table(rowSums(Xmm))
```

The predictor `Type` got "dummified" (remember we spoke about this in lecture 1 or 2). There are now 5 dummy variables each representing one of the levels and the reference level is omitted because it is accounted for in the intercept. Let's make sure this is exactly what's going on.

```{r}
y = cars$Price
Xt = t(Xmm) 
XtX = Xt %*% Xmm
XtXinv = solve(XtX)
b = XtXinv %*% Xt %*% y
b
yhat = Xmm %*% b
e = y - yhat
Rsq = (var(y) - var(e)) / var(y)
Rsq
sqrt(sum(e^2) / (nrow(cars) - 6))
```

And of course the coefficients and $R^2$ are identical to the output from `lm`.

If we want to do a more "pure ANOVA", we can get rid of the intercept and see the $\bar{y}$'s immediately. This is handled in R's formula designation by adding a zero:

```{r}
anova_mod = lm(Price ~ 0 + Type, cars)
coef(anova_mod)
```

Is this correct?

```{r}
mean(cars$Price[cars$Type == "Compact"])
mean(cars$Price[cars$Type == "Large"])
mean(cars$Price[cars$Type == "Midsize"])
mean(cars$Price[cars$Type == "Small"])
mean(cars$Price[cars$Type == "Sporty"])
mean(cars$Price[cars$Type == "Van"])
```

What does $R^2$ look like?

```{r}
summary(anova_mod)$r.squared
```

Remember this from last time? What happened? The $R^2$ calculation in `lm` is not accurate without the intercept. Keep this in mind. 

What does the design matrx (model matrix) look like? we can use the `model.matrix` function to generate the columns of $X$ from the data frame. The argument is the formula we wish to generate the model matrix for. Since model matrices don't require

```{r}
Xmm = model.matrix(~ 0 + Type, cars)
head(Xmm, 20)
table(rowSums(Xmm))
```

Very similar. 

Regressions without an intercept are not recommended. Here's why. What if we were doing two factors? I want a linear model with both Type and Airbags:

```{r}
table(cars$AirBags)
```

Airbags is another nominal categorical variable, this time with three levels.

We invoke the model as follows.

```{r}
anova_mod = lm(Price ~ Type + AirBags, cars)
coef(anova_mod)
summary(anova_mod)$r.squared
summary(anova_mod)$sigma
```

What are interpretations now? What is the "reference level"? It's actually two levels in one: Type = compact and Airbags = Driver \& Passenger. 

A deeper question: can we read off Type = Midsize and AirBags = none? No... this is a modeling "enhancement" we will discuss in a few lectures from now.

If we model it without an intercept,


```{r}
anova_mod = lm(Price ~ 0 + AirBags + Type, cars)
coef(anova_mod)
```

we only get $\bar{y}$'s for the first factor predictor crossed with the reference category of the second. So above `TypeCompact` refers to the average of Type = Compact and Airbags = Driver \& Passenger.

Now let's create a linear model using one categorical predictor and one continuous predictor. The combination is called for historical reasons "Analysis of Covariance" or "ANCOVA" for short.

Let's use `Type` and `Horsepower`:

```{r}
ancova_mod = lm(Price ~ Type + Horsepower, cars)
coef(ancova_mod)
summary(ancova_mod)$r.squared
summary(ancova_mod)$sigma
```

Interpretation of estimated coefficients? Why did $R^2$ increase?.

What's going on the design / model matrix?

```{r}
head(model.matrix(Price ~ Type + Horsepower, cars))
```

Same as model matrix with just `Type`. Since `Horsepower` is continuous, it doesn't get dummified to more features.

What if we went back to the `Type` regression, left out the intercept, dummified and added the intercept back in?

```{r}
Xmm = model.matrix(Price ~ 0 + Type, cars)
Xmm = cbind(1, Xmm)
head(Xmm,20)
```

Are the columns linearly independent? No ... so when we try to get the hat matrix,

```{r}
Xmm %*% solve(t(Xmm) %*% Xmm) %*% t(Xmm)
```

You can't invert a non-invertible matrix!!

What does R do when using the linear model function:

```{r}
coef(lm(cars$Price ~ 0 + Xmm))
```

SOMEHOW: it doesn't complain since it handles the non-invertibility but we do see that the coefficients are busted in some way. Look at the coefficients! One is missing! What is R doing?? 

It's just arbitrarily dropping one (just like we recommended).


## Multivariate linear regression with the Hat Matrix

First let's do the null model to examine what the null hat matrix looks like. In this exercise, we will see that $g_0 = \bar{y}$ is really the OLS solution in the case of no features, only an intercept i.e. $b_0 = \bar{y}$.

```{r}
y = MASS::Boston$medv
mean(y)
```

We can use `lm`.

```{r}
mod = lm(y ~ 1)
coef(mod)
mod$fitted.values
```


Let's do a simple example of projection. Let's project $y$ onto the intercept column, the column of all 1's. What do you think will happen?

```{r}
ones = rep(1, length(y))
H = ones %*% t(ones) / sum(ones^2)
H[1 : 5, 1 : 5]
#in fact
unique(c(H))
1 / 506
```

The whole matrix is just one single value for each element! What is this value? It's 1 / 506 where 506 is $n$. So what's going to happen?

```{r}
y_proj_ones = H %*% y
head(y_proj_ones)
```

Projection onto the space of all ones makes the null model ($g = \bar{y}$). It's the same as the model of response = intercept + error, i.e. $y = \mu_y + \epsilon$. The OLS intercept estimate is clearly $\bar{y}$.

```{r}
y = MASS::Boston[, 14]
X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
Xt = t(X)
XtXinv = solve(Xt %*% X)
b = XtXinv %*% t(X) %*% y
b
```

We can compute all predictions:

```{r}
H = X %*% XtXinv %*% t(X)
dim(h)
yhat = H %*% y
head(yhat)
```

Can you tell this is projected onto a 13 dimensional space from a 506 dimensional space? Not really... but it is...

Now let's project over and over...

```{r}
head(H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% y)
```

Same thing! Once you project, you're there. That's the idempotency property of the matrix $H$.

Let's make sure that it really does represent the column space of $X$. Let's try to project different columns of $X$:

```{r}
head(X[, 1, drop = FALSE])
head(H %*% X[, 1, drop = FALSE])

head(X[, 2, drop = FALSE])
head(H %*% X[, 2, drop = FALSE])

head(X[, 3, drop = FALSE])
head(H %*% X[, 3, drop = FALSE]) #why?? Numerical error...

head(X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17)
head(H %*% (X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17))

#etc....
```

We can calculate the residuals:

```{r}
e = y - yhat
head(e)
I = diag(nrow(X))
e = (I - H) %*% y
head(e)
```

Let's do that projection over and over onto the complement of the column space of $X$:

```{r}
head((I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% y)
```

