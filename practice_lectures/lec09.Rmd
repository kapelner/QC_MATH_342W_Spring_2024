---
title: "Practice Lecture 9 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

## ANCOVA

Now let's create a linear model using one categorical predictor and one continuous predictor. The combination is called for historical reasons "Analysis of Covariance" or "ANCOVA" for short.

Let's use `Type` and `Horsepower`:

```{r}
ancova_mod = lm(Price ~ Type + Horsepower, cars)
coef(ancova_mod)
summary(ancova_mod)$r.squared
summary(ancova_mod)$sigma
```

Interpretation of estimated coefficients? Why did $R^2$ increase?.

What's going on the design / model matrix?

```{r}
head(model.matrix(Price ~ Type + Horsepower, cars))
```

Same as model matrix with just `Type`. Since `Horsepower` is continuous, it doesn't get dummified to more features.

What if we went back to the `Type` regression, left out the intercept, dummified and added the intercept back in?

```{r}
Xmm = model.matrix(Price ~ 0 + Type, cars)
Xmm = cbind(1, Xmm)
head(Xmm,20)
```

Are the columns linearly independent? No ... so when we try to get the hat matrix,

```{r}
Xmm %*% solve(t(Xmm) %*% Xmm) %*% t(Xmm)
```

You can't invert a non-invertible matrix!!

What does R do when using the linear model function:

```{r}
coef(lm(cars$Price ~ 0 + Xmm))
```

SOMEHOW: it doesn't complain since it handles the non-invertibility but we do see that the coefficients are busted in some way. Look at the coefficients! One is missing! What is R doing?? 

It's just arbitrarily dropping one (just like we recommended).


## Multivariate linear regression with the Hat Matrix

First let's do the null model to examine what the null hat matrix looks like. In this exercise, we will see that $g_0 = \bar{y}$ is really the OLS solution in the case of no features, only an intercept i.e. $b_0 = \bar{y}$.

```{r}
y = MASS::Boston$medv
mean(y)
```

We can use `lm`.

```{r}
mod = lm(y ~ 1)
coef(mod)
mod$fitted.values
```


Let's do a simple example of projection. Let's project $y$ onto the intercept column, the column of all 1's. What do you think will happen?

```{r}
ones = rep(1, length(y))
H = ones %*% t(ones) / sum(ones^2)
H[1 : 5, 1 : 5]
#in fact
unique(c(H))
1 / 506
```

The whole matrix is just one single value for each element! What is this value? It's 1 / 506 where 506 is $n$. So what's going to happen?

```{r}
y_proj_ones = H %*% y
head(y_proj_ones)
```

Projection onto the space of all ones makes the null model ($g = \bar{y}$). It's the same as the model of response = intercept + error, i.e. $y = \mu_y + \epsilon$. The OLS intercept estimate is clearly $\bar{y}$.

```{r}
y = MASS::Boston[, 14]
X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
Xt = t(X)
XtXinv = solve(Xt %*% X)
b = XtXinv %*% t(X) %*% y
b
```

We can compute all predictions:

```{r}
H = X %*% XtXinv %*% t(X)
dim(h)
yhat = H %*% y
head(yhat)
```

Can you tell this is projected onto a 13 dimensional space from a 506 dimensional space? Not really... but it is...

Now let's project over and over...

```{r}
head(H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% y)
```

Same thing! Once you project, you're there. That's the idempotency property of the matrix $H$.

Let's make sure that it really does represent the column space of $X$. Let's try to project different columns of $X$:

```{r}
head(X[, 1, drop = FALSE])
head(H %*% X[, 1, drop = FALSE])

head(X[, 2, drop = FALSE])
head(H %*% X[, 2, drop = FALSE])

head(X[, 3, drop = FALSE])
head(H %*% X[, 3, drop = FALSE]) #why?? Numerical error...

head(X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17)
head(H %*% (X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17))

#etc....
```

We can calculate the residuals:

```{r}
e = y - yhat
head(e)
I = diag(nrow(X))
e = (I - H) %*% y
head(e)
```

Let's do that projection over and over onto the complement of the column space of $X$:

```{r}
head((I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% y)
```

