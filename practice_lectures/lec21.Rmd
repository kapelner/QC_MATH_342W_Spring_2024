---
title: "Practice Lecture 21 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


# Random Forests

Can we do any better? YES. As you saw, the variance terms can be shrunk further the more decorrelated the trees become. We do this now by introducing randomness into the splits by choosing only a subset of the features to split on randomly. The trees are then grown as normal. Then the we model average many trees via bagging. And that's random forests!

Quick demo with the diamonds and oob validation:

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF, tidyverse, magrittr)
rm(list = ls())

seed = 1984
set.seed(seed)
n_samp = 2000
#note: trees are perfect for ordinal variables! Since it only needs an ordering to do the splits
#and thus it is immune to different encodings so, just use the standard 1, 2, 3, ...
#thus there is no need here to cast the ordinal variables cut, color, clarity to nominal variables
diamonds_samp = ggplot2::diamonds %>% sample_n(n_samp)
y = diamonds_samp$price
X = diamonds_samp %>% dplyr::select(-price)

num_trees = 1000
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
mod_bag

#now for apples-apples, build the RF on the same bootstrap data for each tree
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices)
mod_rf

# pacman::p_load(randomForest)
# mod_rf = randomForest(X, y, num_trees = num_trees)
# mod_rf

cat("gain: ", (mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, "%\n")
```

This was a fail. RF is sensitive to the number of features samples (the hyperparameter mtry). Really, this hyperparameter should be selected using a grid search. We will do this on lab. Here is an example using the best value of `mtry`:


```{r}
#there are 9 total features
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices, mtry = 8)
mod_rf

cat("gain: ", (mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, "%\n")
```

This gain is not so impressive. But it is real (as we showed theoretically) and it's for free... so why not take it?

If `mtry` is small, the gain may be so small in the rho-multiple on the variance term that it doesn't outweigh the increase in bias. Thus, we underfit a little bit. Thus it's better to stay with just bagging. Here it is on the boston housing data:

```{r}
rm(list = ls())
y = MASS::Boston$medv
X = MASS::Boston
X$medv = NULL
seed = 1984
num_trees = 1000
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
mod_bag
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices, mtry = 11)
mod_rf
cat("oob rmse gain:", round((mod_bag$rmse_oob - mod_rf$rmse_oob) / mod_bag$rmse_oob * 100, 3), "%\n")
```

Slightly more impressive. But again it is for free. 

What about for classification models?

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
n_samp = 2000

seed = 1984
set.seed(seed)
adult_samp = na.omit(adult) %>% sample_n(n_samp)
y = adult_samp$income
X = adult_samp %>% dplyr::select(-income)

num_trees = 500
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
#default mtry for classification models is floor(sqrt(p))
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices) 
mod_bag$misclassification_error
mod_rf$misclassification_error

cat("gain: ", (mod_bag$misclassification_error - mod_rf$misclassification_error) / mod_bag$misclassification_error * 100, "%\n")
```

Very nice... and can likely be improved with optimizing mtry.

And on letters:

```{r}
rm(list = ls())
pacman::p_load(mlbench, skimr)
data(LetterRecognition)
LetterRecognition = na.omit(LetterRecognition) #kill any observations with missingness
#skim(LetterRecognition)
?LetterRecognition

n_samp = 2000
seed = 1984
set.seed(seed)
letters_samp = LetterRecognition %>% sample_n(n_samp)
y = letters_samp$lettr
X = letters_samp %>% dplyr::select(-lettr)

num_trees = 500
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)

#default mtry for classification models is floor(sqrt(p))
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed, bootstrap_indices = mod_bag$bootstrap_indices) 
mod_bag$misclassification_error
mod_rf$misclassification_error

cat("gain: ", (mod_bag$misclassification_error - mod_rf$misclassification_error) / mod_bag$misclassification_error * 100, "%\n")
```

Very nice again... and can likely be improved with optimizing mtry.

There are very real gains for RF over bagging and these are most pronounced in classification models.


