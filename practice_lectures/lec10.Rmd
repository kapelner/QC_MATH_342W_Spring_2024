---
title: "Practice Lecture 10 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


## Multivariate linear regression with the Hat Matrix

First let's do the null model to examine what the null hat matrix looks like. In this exercise, we will see that $g_0 = \bar{y}$ is really the OLS solution in the case of no features, only an intercept i.e. $b_0 = \bar{y}$.

```{r}
y = MASS::Boston$medv
length(y) #sample size, n
mean(y) #ybar
```

We can use `lm`.

```{r}
mod = lm(y ~ 1)
coef(mod)
mod$fitted.values
```


Let's do a simple example of projection. Let's project $y$ onto the intercept column, the column of all 1's. What do you think will happen?

```{r}
ones = rep(1, length(y))
H = ones %*% t(ones) / sum(ones^2)
H[1 : 5, 1 : 5]
#in fact
unique(c(H))
1 / 506
```

The whole matrix is just one single value for each element! What is this value? It's 1 / 506 where 506 is $n$. So what's going to happen?

```{r}
y_proj_ones = H %*% y
head(y_proj_ones)
```

Projection onto the space of all ones makes the null model ($g = \bar{y}$). It's the same as the model of response = intercept + error, i.e. $y = \mu_y + \epsilon$. The OLS intercept estimate is clearly $\bar{y}$.

```{r}
y = MASS::Boston[, 14]
X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
Xt = t(X)
XtXinv = solve(Xt %*% X)
b = XtXinv %*% t(X) %*% y
b
```

We can compute all predictions:

```{r}
H = X %*% XtXinv %*% t(X)
dim(h)
yhat = H %*% y
head(yhat)
```

Can you tell this is projected onto a 13 dimensional space from a 506 dimensional space? Not really... but it is...

Now let's project over and over...

```{r}
head(H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% H %*% y)
```

Same thing! Once you project, you're there. That's the idempotency property of the matrix $H$.

Let's make sure that it really does represent the column space of $X$. Let's try to project different columns of $X$:

```{r}
head(X[, 1, drop = FALSE])
head(H %*% X[, 1, drop = FALSE])

head(X[, 2, drop = FALSE])
head(H %*% X[, 2, drop = FALSE])

head(X[, 3, drop = FALSE])
head(H %*% X[, 3, drop = FALSE]) #why?? Numerical error...

head(X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17)
head(H %*% (X[, 1, drop = FALSE] * 3 + X[, 2, drop = FALSE] * 17))

#etc....
```

We can calculate the residuals:

```{r}
e = y - yhat
head(e)
I = diag(nrow(X))
e = (I - H) %*% y
head(e)
```

Let's do that projection over and over onto the complement of the column space of $X$:

```{r}
head((I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% (I - H) %*% y)
```



## Eigendecomposition in R and the Projection Matrix

```{r}
B = array(seq(1, 4), dim = c(2, 2))
B
eigen_decomp = eigen(B)
V = eigen_decomp$vectors
V
v1 = V[, 1, drop = FALSE]
v2 = V[, 2, drop = FALSE]
lambdas = eigen_decomp$values
lambdas
lambda1 = lambdas[1]
lambda2 = lambdas[2]
B %*% v1
lambda1 * v1
B %*% v2 
lambda2 * v2
B %*% v2 == lambda2 * v2 #why not?
#diagonolization
V %*% diag(lambdas) %*% solve(V)
```

Let's take a look at a projection matrix:

```{r}
X = model.matrix(medv ~ ., MASS::Boston)
head(X)
H = X %*% solve(t(X) %*% X) %*% t(X)
H[1:5,1:5]
dim(H)
eigen_decomp = eigen(H)
lambdas = eigen_decomp$values
head(lambdas, 50)
lambdas = as.numeric(eigen_decomp$values) #coerce to real numbers (good enough approximation)
head(lambdas, 50)
lambdas_rounded = round(lambdas, 10)
head(lambdas_rounded, 50)
table(lambdas_rounded)
sum(lambdas_rounded) #i.e. the rank(H) = # cols in X
```

It turns out all eigenvalues of a projection matrix are either 0 or 1. Why is this? It makes sense since:

H 1-vec = 1-vec
H x_{dot 1} = x_{dot 1}
H x_{dot 2} = x_{dot 2}
.
.
.
H x_{dot p} = x_{dot p}

All these p+1 eigenvalues of H are 1 and there are p+1 of them. Further,

```{r}
sum(diag(H))
```

The trace (sum of the diagonal entries) is also = p+1 i.e. the rank[X]. This is true since the trace of a matrix is always equal to the sum of the eigenvalues since tr(H) = tr(V^-1 D V) = tr(V^-1 V D) = tr(D) = sum lambda_i which in this case is rank[X].

What are the eigenvectors? Only the first ncol(X) = 14 reconstruct the space. Are they the same as the X columns?

```{r}
V = eigen_decomp$vectors
V = matrix(as.numeric(V), ncol = ncol(V))
dim(V)
V[1 : 5, 1 : ncol(X)]

V_X_angles = matrix(NA, ncol(X), ncol(X))
for (i in 1 : ncol(X)){
  for (j in 1 : ncol(X)){
    if (j > i){
      V_X_angles[i, j] = acos(V[, i] %*% X[, j] / sqrt(sum(V[, i]^2) * sum(X[, j]^2))) * 180 / pi
    }
  }
}
V_X_angles
```

No... and they don't need to be. They just need to represent the same space i.e. their columns need to span the same 14-dimensional subspace. You can check this by just comparing the orthogonal projection matrix which is unique for each subspace.

```{r}
V_X = V[, 1 : ncol(X)]
H_V = V_X %*% solve(t(V_X) %*% V_X) %*% t(V_X)
dim(H_V)
testthat::expect_equal(c(H_V), c(H))
```

And they're the same as expected.

# QR Decomposition

Let's go back to the Boston data and regenerate all our quantities:

```{r}
y = MASS::Boston$medv
ybar = mean(y)
SST = sum((y - ybar)^2)
SST


X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
n = nrow(X)
p_plus_one = ncol(X)
Xt = t(X)
XtXinv = solve(Xt %*% X)
b = XtXinv %*% Xt %*% y
b
yhat = X %*% b
head(yhat)
# e = y - yhat
# SSE = sum(e^2)
SSR = sum((yhat - ybar)^2)
SSR
Rsq = SSR / SST
Rsq
```

Now let's do the QR decomposition and see if the projections work.

```{r}
nrow(X)
p_plus_one
qrX = qr(X)
class(qrX)
Q = qr.Q(qrX)
R = qr.R(qrX)
dim(Q)
dim(R)
Matrix::rankMatrix(Q)
Matrix::rankMatrix(R)

head(Q[, 1], 50)
head(Q[, 2], 50)
1 / sqrt(nrow(X))
sum(Q[, 1]^2) #normalized?
sum(Q[, 2]^2) #normalized?
Q[, 1] %*% Q[, 2] #orthogonal?
Q[, 7] %*% Q[, 13] #orthogonal?

Qt = t(Q)
yhat_via_Q = Q %*% Qt %*% y
head(yhat)
head(yhat_via_Q)
testthat::expect_equal(c(yhat), c(yhat_via_Q)) #needed to vectorize to make dimensions equal
```

Can we get the $b$ vector from the $Q$ matrix?

```{r}
solve(R) %*% Qt %*% y
b_Q = Qt %*% y
b_Q
head(Q %*% b_Q)
head(X %*% b)
```

Nope - this is not the same! Why not?


Nope - this is not the same! Why not?

Each dimension gives one piece of SSR and thus one piece of R^2 i.e. SSR = SSR_1 + ... + SSR_p and R^2 = R^2_1 + ... + R^2_p

Our definition of SSR removed the ybar i.e. the contribution of the intercept. So we will do so here. That is the first column of $Q$. Now we add up all the features besides the intercept

```{r}
partial_SSRs = array(NA, p_plus_one)
for (j in 2 : p_plus_one){
  qj = Q[, j, drop = FALSE]
  yhat_j = qj %*% t(qj) %*% y #the projection onto the jth dimension of Q
  partial_SSRs[j] = sum(yhat_j^2)
}
round(partial_SSRs)
SSR
sum(partial_SSRs, na.rm = TRUE)
SST
partial_Rsqs = partial_SSRs / SST
round(partial_Rsqs, 2)
sum(partial_Rsqs, na.rm = TRUE)
```

Some dimensions in this subspace matter more than others. We can do approximately the same regression with less than p features. Let's try this:

```{r}
partial_Rsqs_sorted = sort(partial_Rsqs, decreasing = TRUE)
partial_Rsqs_sorted_cumul = cumsum(partial_Rsqs_sorted)
partial_Rsqs_sorted_cumul
#sort Q by Rsq
Qsorted = Q[, order(partial_Rsqs, na.last = FALSE, decreasing = TRUE)]
#let's take the first 8
Qreduced = Qsorted[, 1 : 8]
mod = lm(y ~ Qreduced)
summary(mod)$r.squared
```

Why was the first column of `Qsorted` dropped?

# Correlation zero means orthogonality

Let's generate some fake data. In this example we'll have one predictor which will be orthogonal to the centered response. We enforce the response to be centered by adding a column of 1's:

```{r}
n = 100; p = 2
Q = qr.Q(qr(cbind(1, matrix(rnorm(n * p), nrow = n))))
y = Q[, p + 1]
x = Q[, 2]
```

Let's make sure it's orthogonal:

```{r}
x %*% y
```

If they're orthogonal and y is mean-centered, what is the correlation?

```{r}
cor(x, y)
```

If the correlation is 0, what is $b_1$, the slope? It has to be zero. Thus $b_0$ has to be $bar{x}$. Since x was also orthogonalized to the vector of 1's, it's centered and hence has average = 0. So both intercept and slope are 0:

What is $b$?

```{r}
mod = lm(y ~ x)
coef(mod)
```

What is $R^2$? Since $x$ and $y$ are orthogonal... a projection onto the colspace of $X$ gets annhilated.

```{r}
summary(mod)$r.squared
```

# Random correlations are non-zero

```{r}
set.seed(1984)
n = 100
x = rnorm(n)
x = x - mean(x)
y = rnorm(n)
y = y - mean(y)
x
y
```

In this setup, $x$ and $y$ are centered Gaussian random vectors. Are they orthogonal?

```{r}
x %*% y
theta_in_rad = acos(x %*% y / sqrt(sum(x^2) * sum(y^2)))
theta_in_rad
theta_in_rad * 180 / pi
abs(90 - theta_in_rad * 180 / pi)
```

Nope... what about correlated?

```{r}
cor(x, y)
cor(x, y)^2
```

They *nearly* uncorrelated but they still have some correlation. How is this possible? 

There is "random chance"" AKA "chance capitalization"!

What about the best fitting line?

```{r}
mod = lm(y ~ x)
coef(mod)
```

Slope is about 0.09 which is small but non-zero.

What is $R^2$? Since $x$ and $y$ are nearly orthogonal... a projection onto the colspace of $X$ gets nearly annhilated...

```{r}
summary(mod)$r.squared
```

...but not entirely annhilated. Lesson learned: random noise can be correlated with the response $y$ and give you the illusion of fit!
