---
title: "Practice Lecture 24 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

We will examine K-means clustering algorithm (Hartigan and Wong, 1979)
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=Algorithm+AS+136%3A+A+k-means+clustering+algorithm&btnG=

using the airbnb listing data found from https://insideairbnb.com/ specifically, the recent
listings in NYC. We first load the data:

```{r}
pacman::p_load(tidyverse)
listings_raw = read_csv("https://data.insideairbnb.com/united-states/ny/new-york-city/2024-04-06/data/listings.csv.gz")
head(listings)
```

There are obviously useless variables. And there are textual variables. If we were doing a more complete analysis, we would convert the text into features. We also are interested not grouping by geography (which is trivial) but instead group by type and quality. Thus, for now, we retain only a few features:

```{r}
#more complicated analysis
listings = listings_raw %>% select(host_listings_count, host_total_listings_count, host_verifications, host_has_profile_pic, host_identity_verified, room_type, accommodates, bedrooms, beds, number_of_reviews, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, reviews_per_month)

#simpler analysis
listings = listings_raw %>% select(host_listings_count, host_total_listings_count, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, reviews_per_month)
```

Now we take a look at the data:

```{r}
skimr::skim(listings)
```

Let's convert characters to factors and logicals to numeric:

```{r}
#for advanced analysis only
# listings = listings %>% 
#   mutate(host_verifications = factor(host_verifications)) %>%
#   mutate(room_type = factor(room_type)) %>%
#   mutate(host_has_profile_pic = as.numeric(host_has_profile_pic)) %>%
#   mutate(host_identity_verified = as.numeric(host_identity_verified))
# skimr::skim(listings)
```

We've got some missingness. So let's create some dummies:

```{r}
M = apply(is.na(listings), 2, as.numeric)
colnames(M) = paste("is_missing_", colnames(listings), sep = "")
M = M[, colSums(M) > 0]
```

and do the imputation very approximately (otherwise we'd be here all day)

```{r}
pacman::p_load(missForest)
listings_imp = missForest(data.frame(listings), sampsize = rep(500, 100), ntree = 100)$ximp
```

Now create the design matrix:

```{r}
X = cbind(
  model.matrix(~ 0 + ., listings_imp),
  M
)
skimr::skim(X)
```

Now we normalize the metrics:

```{r}
Xnorm = apply(X, 2, function(xj){(xj - mean(xj))/ sd (xj)})
skimr::skim(Xnorm)
```

Now we're finally ready to roll. `kmeans` is actually included in base R without the need for a package. We'll define Kmax to be 5 for now. We'll let it converge in 300 iterations and we'll only use one starting position:

```{r}
kmeans_info = kmeans(Xnorm, centers = 5, iter.max = 300, nstart = 1)
```

Now let's look at the clusters (use X here, not Xnorm to see the real data)

```{r}
aggr_res = aggregate(X, by = list(cluster = kmeans_info$cluster), mean)
as_tibble(aggr_res) %>% arrange(host_listings_count)
```

Let's now try to identify the optimal number of clusters. Here we use nstart = 10 to get a smoother result over K. However, this does take 10x as long to crunch! It will throw some warnings (which we'll ignore).

```{r}
Kmax = 50
tot_SSE_by_K = array(NA, Kmax) 
for (K in 1 : Kmax){
  tot_SSE_by_K[K] = kmeans(Xnorm, centers = K, iter.max = 300, nstart = 10)$tot.withinss
}

#"scree" plot
ggplot(data.frame(K = 1 : Kmax, tot_SSE = tot_SSE_by_K)) +
  aes(x = K, y = tot_SSE) +
  geom_point(size = 4) +
  geom_line() +
  xlab('Number of clusters (K)') +
  ylab("total SSE across all K clusters")

ggplot(data.frame(K = 2 : Kmax, tot_SSE = diff(tot_SSE_by_K))) +
  aes(x = K, y = tot_SSE) +
  geom_point(size = 4) +
  geom_line() +
  ylab("diff SSE between K, K-1 clusters") + 
  xlab('Number of clusters (K)')
```

Now we visually pick the point where the derivative is getting noticeably smaller. The diff plot helps. Maybe K = 6? Or K = 8? It's an art, not a science.



# Deep Learning for Binary Image Classification via Convolutional Neural Nets (CNNs)

Frist of all, the list of common activation functions:
https://en.wikipedia.org/wiki/Activation_function


I'm following an example located here:
https://blogs.rstudio.com/ai/posts/2020-10-19-torch-image-classification/

We first load the packages and some datasets:

```{r}
rm(list = ls())
pacman::p_install_gh("mlverse/torch")
pacman::p_install_gh("mlverse/torchdatasets")
Sys.setenv(CUDA = "cpu")
install_torch()
pacman::p_load(torch, torchvision, luz, torchdatasets, tidyverse, pins)
```

Now we download the dog-cat database:

```{r}
data_dir = "~/Downloads/dogs-vs-cats"
#do the following once only, then comment out
dogs_vs_cats_dataset(data_dir, download = TRUE, transform = train_transforms)
```

Now let's download the data then (a) transform its representation to a tensor (b) resize each image to be standard 224x224 and (c) normalize the pixel values.

```{r}
mean_intensity = rep(0.5, 3)
sd_intensity = rep(0.5, 3)
full_data_set = dogs_vs_cats_dataset(
  data_dir,
  transform = function(img) {
    img %>%
      torchvision::transform_to_tensor() %>%
      torchvision::transform_resize(size = c(224, 224)) %>% 
      torchvision::transform_normalize(mean_intensity, sd_intensity)
  },
  target_transform = function(x) as.double(x) - 1
)
```

Now we split the data into three splits:

```{r}
set.seed(1)
train_idx =  sample(1 : length(full_data_set), size = 0.6 * length(full_data_set))
select_idx = sample(setdiff(1 : length(full_data_set), train_idx), size = 0.2 * length(full_data_set))
test_idx =   setdiff(1 : length(full_data_set), union(train_idx, select_idx))

D_train =  dataset_subset(full_data_set, indices = train_idx)
D_select = dataset_subset(full_data_set, indices = select_idx)
D_test =   dataset_subset(full_data_set, indices = test_idx)

batch_size = 8
num_workers = 2
train_batcher =  dataloader(D_train, batch_size = batch_size, shuffle = TRUE, num_workers = num_workers)
```

Let's take a look at some images before we begin to do the modeling. To do so, we have to jump through lots of hoops:

```{r}
images = aperm(as_array(train_batcher$.iter()$.next()[[1]]), perm = c(1, 3, 4, 2))
images = mean_intensity + sd_intensity * images + mean
images = images * 255
images[images > 255] = 255
images[images < 0] = 0

images %>%
  purrr::array_tree(1) %>%
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x); title(.y)})
```

Here are some good illustrations about how convolutions work:
https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/image_classification_1.html

And now what architecture should be used? Many options! Classic is "alexnet" from 2012 (although outdated now)
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=ImageNet+classification+with+deep+convolutional+neural+networks%22&btnG=

Here is a diagram of the layers:
https://medium.com/analytics-vidhya/concept-of-alexnet-convolutional-neural-network-6e73b4f9ee30

We can fit initialize the model structure of alexnet below:

```{r}
net = torch::nn_module(
  initialize = function(output_size) {
    self$model = model_alexnet(pretrained = TRUE)

    for (par in self$parameters) {
      par$requires_grad_(FALSE)
    }

    self$model$classifier = nn_sequential(
      nn_dropout(0.5),
      nn_linear(9216, 512),
      nn_relu(),
      nn_linear(512, 256),
      nn_relu(),
      nn_linear(256, output_size)
    )
  },
  forward = function(x) {
    self$model(x)[,1]
  }
)
```

We initialize some hyperparameters:

```{r}
objective_function = nn_bce_with_logits_loss()
gradient_descient_algorithm = optim_adam
eta = 0.01
num_iter = 3 #this is low
torch_set_num_threads(10)
```

Why is number of iterations only 3?? Well, in this demo, we are cheating since we are using a "pretrained" alexnet (i.e. it was already trained for a ton of images and fitted to their weights). How? We simply download the entire structure with the weights from google's servers. Then, with that structure as our starting point (i.e. standing on the shoulders of giants), we only need a few more iterations.

```{r}
cnn_mod = net %>%
  setup(
    loss = objective_function,
    optimizer = gradient_descient_algorithm,
    metrics = list(
      luz_metric_binary_accuracy_with_logits()
    )
  ) %>%
  set_hparams(output_size = 1) %>%
  set_opt_hparams(lr = eta) %>%
  fit(D_train, epochs = num_iter, valid_data = D_select)
```

This is going really slow since we are using the CPU (not GPUs). The speedup reported are usually 10x. However, GPUs are more expensive to buy and more expensive to run (they use more electricity).

Now let's take a look at what the oos predictions look like:

```{r}
yhat = predict(cnn_mod, D_test)
head(yhat)
```

Even prediction takes time! Why, it has to do all the convolutions and all the forward fed computations. Note that yhat is in logodds. So let's convert it to probability estimates:

```{r}
phat = torch_sigmoid(yhat)
head(phat)
```

And how did we do? Let's use default thresholding method:

```{r}
y_test = array(NA, length(D_test))
for (i in 1 : length(D_test)){
  y_test[i] = D_test[[i]][[2]]
}

oos_conf_tab = table(y_test, as.numeric(phat) > 0.5)
oos_conf_tab
oos_conf_tab / sum(oos_conf_tab)
#misclassification error:
(oos_conf_tab[1, 2] + oos_conf_tab[2, 1]) / sum(oos_conf_tab)
```

Note that convolutional layers are not really layers in the traditional sense as we described in class; they're more of a means of "featurizing". Once you use these convolutional computations you eventually wind up with real-valued covariates. You can then feed them into other algorithms e.g. random forest:
https://www.nature.com/articles/s41598-022-15374-5

But traditionally, 


Now we write some helper functions that will transform the image data (it needs to be transformed from an R array representation to a "tensor" representation and the pixels need to be normalized). 

The training data also gets "augmented" meaning we take the original images and manipulate them slightly to broaden the examples e.g. randomly resize them (so dogs and cats appear bigger / smaller), randomly jitter the colors (as lighting will be different), flip the image from left-right

```{r}
train_transforms = function(img) {
  img %>%
    # first convert image to tensor (this is not data augmentation)
    transform_to_tensor() %>%
    # data augmentation
    transform_random_resized_crop(size = c(224, 224)) %>%
    # data augmentation
    transform_color_jitter() %>%
    # data augmentation
    transform_random_horizontal_flip() %>%
    # normalize according to what is expected by resnet
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}
```

As for the select and test set, we don't augment, but we still need to do the resize and crop and normalize the pixels.

```{r}
select_transforms = function(img) {
  img %>%
    transform_to_tensor() %>%
    transform_resize(256) %>%
    transform_center_crop(224) %>%
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}
test_transforms = select_transforms
```