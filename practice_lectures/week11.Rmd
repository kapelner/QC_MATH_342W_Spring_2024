---
title: "Practice Lectures Week 11 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "Apr 25, 2022"
---


## Data "Munging" with Dplyr and data.table

"Data munging", sometimes referred to as "data wrangling", is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. A data wrangler is a person who performs these transformation operations. -[Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling).

Half of what a data scientist does is cleaning data, visualizing data and wrangling it. In the process you learn all about your dataset and you're on a higher level when it comes time to build prediction models.

The packages `dplyr` and `data.table` offer many conveninent functions to manipulate, clean, and otherwise wrangle data. Note: all the wrangling we're going to see *can* be done with base R (see previous notes on the `data.frame` object) but it would be *very very very very* annoying and *very very very very* slow.

I will quickly compare and contrast `dplyr` and `data.table` before you see it inside actual code.

* `dplyr` works really nicely with the piping chain as you "begin" the manipulation with the dataset and then iteratively pipe in step 1, step 2, etc until you wind up with what end product you would like. This makes `dplyr` very readable but very verbose - lots of lines of code. 
* On the flip side, `data.table` essentially wrote a new data wrangling language so it's a harder learning curve but it's very compact - very few lines of code.
* `data.table` is blazing fast and kills `dplyr` in performance and I'm pretty sure it even beats Python in performance (someone please check this). So in the era of "big data", I think this is the winner even though it is much harder to learn.
* I believe `dplyr` is more popular in the real world and thus has more cache to put on your CV. But this is constantly in flux!

For all labs and the final project, you are recommended to pick one you want to use and go with it. For the exams, I will write code in both (if need be) to not penalize / reward a student who picked one over the other.

Here is a nice [translation guide](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) between `dplyr` and `data.table`. We will be learning them in tandem. I could've split this into two units but I decided against it because (1) it is good to see the same functionality side-by-side and (2) this is really just one concept.

```{r}
pacman::p_load(tidyverse, magrittr) #tidyverse is shorthard for dplyr, ggplot2, tidyr, readr and a bunch of other packages recommended for the "full" dplyr experience. I'm using magrittr for special pipe operations later.
pacman::p_load(data.table)
```

First, recall what pipe format means! We're going to need to know this well for what's coming next...

```{r}
set.seed(1984)
mean(head(round(sample(rnorm(1000), 100), digits = 2)))

set.seed(1984)
rnorm(1000) %>% #the pipe operator
  sample(100) %>% 
  round(digits = 2) %>% #the first argument is passed in automatically.
  head %>%
  mean
```

Note that `data.table` is automatically multithreaded. Read [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/setDTthreads).

```{r}
getDTthreads()
```


We first instantiate the upgraded data.frame objects in both libraries:

```{r}
diamonds_tbl = as_tibble(diamonds) #not necessary to cast because dplyr does the conversion automatically after using any dplyr function
diamonds_dt = data.table(diamonds) #absolutely necessary
```

What happens during the data frame conversion?

```{r}
class(diamonds_tbl)
class(diamonds_dt)
```

Note how these are implemented as class extensions of R's `data.frame` as to allow for background compatibility and not break the API. They have nicer ways of showing the data:

```{r}
diamonds_tbl #run this in the console, not inside the chunk
diamonds_dt #run this in the console, not inside the chunk
```

Beginning with the simplest munging tasks, subsetting rows:

```{r}
diamonds_tbl %>% 
  slice(1 : 5)

diamonds_dt[1 : 5]
```

And subsetting columns:

```{r}
diamonds_tbl %>% 
  select(cut, carat, price) #these three only in this order

diamonds_dt[, .(cut, carat, price)]

diamonds_tbl %>% 
  select(carat, price, cut) #these three only in another order

diamonds_dt[, .(carat, price, cut)]

diamonds_tbl %>% 
  select(-x) #drop this feature

diamonds_dt[, !"x"]
#diamonds_dt[, x := NULL] #mutating function (overwrites the data frame)

diamonds_tbl %>% 
  select(-c(x, y, z)) #drop these features
diamonds_tbl %>% 
  select(-x, -y, -z) #drop these features

diamonds_dt[, !c("x", "y", "z")]
```

How about will rename a column

```{r}
diamonds_tbl %>% 
  rename(weight = carat, price_USD = price)

diamonds_dt_copy = copy(diamonds_dt)
setnames(diamonds_dt_copy, old = c("carat", "price"), new = c("weight", "price_USD")) #the `setnames` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

If you want to rearrange the columns...

```{r}
#In dplyr you pretend to select a subset and then ask for everything else:
diamonds_tbl %>% 
  select(carat, price, cut, everything()) #these three in this order first then everything else
# diamonds_tbl %>% 
#   select(-carat, everything()) #move carat last (first drop it, and then add it back in with everything)

diamonds_dt_copy = copy(diamonds_dt)
setcolorder(diamonds_dt_copy, c("carat", "price", "cut")) #as before, the `setcolorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

Sorting the rows by column(s):

```{r}
diamonds_tbl %>%
  arrange(carat) #default is ascending i.e. lowest first

diamonds_dt[order(carat)]
diamonds_dt_copy = copy(diamonds_dt)
setorder(diamonds_dt_copy, carat) #as before, the `setorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)

diamonds_tbl %>%
  arrange(desc(carat)) #switch to descending, i.e. highest first

diamonds_dt[order(-carat)] #and you can do this with `setorder` too

diamonds_tbl %>%
  arrange(desc(color), clarity, cut, desc(carat)) #multiple sorts - very powerful

diamonds_dt[order(-color, clarity, cut, -carat)] #and you can do this with `setorder` too
```

The filter method subsets the data based on conditions:

```{r}
diamonds_tbl %>%
  filter(cut == "Ideal")

diamonds_dt[cut == "Ideal"]

diamonds_tbl %>%
  filter(cut == "Ideal") %>%
  filter(depth < 65) %>%
  filter(x * y * z > 20)
diamonds_tbl %>%
  filter(cut == "Ideal" & depth < 65 & x * y * z > 20)

diamonds_dt[cut == "Ideal" & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter((cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[(cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter(cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20]
```

How about removing all rows that are the same?

```{r}
diamonds_tbl
diamonds_tbl %>%
  distinct

unique(diamonds_dt)

#nice function from data.table:
uniqueN(diamonds$carat) 
#273 < 53940 i.e. there's only a few weight measurements that are possible... let's only keep one from each unique carat value

diamonds_tbl %>%
  distinct(carat, .keep_all = TRUE) #keeps the first row for each unique weight measurement

unique(diamonds_dt, by = "carat")
```

Sampling is easy

```{r}
diamonds_tbl %>%
  sample_n(7)

diamonds_dt[sample(.N, 7)] #.N is a cool function: it is short for `nrow(dt object)`

diamonds_tbl %>%
  sample_frac(1e-3)

diamonds_dt[sample(.N, .N * 1e-3)] #.N is a cool function: it is short for `nrow(dt object)
```


Now for some real fun stuff. Let's create new features with the `mutate` function.

```{r}
diamonds_tbl %>%
  mutate(volume = x * y * z) #adds a new column keeping the old ones (this was an exam problem in a previous year)

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, volume := x * y * z]
diamonds_dt2

diamonds_tbl %>%
  mutate(price_per_carat = price / carat) %>%
  arrange(desc(price_per_carat))

diamonds_dt2[, price_per_carat := price / carat]
diamonds_dt2[order(-price_per_carat)]
rm(diamonds_dt2)
```

Or rewrite old ones.

```{r}
diamonds_tbl %>%
  mutate(cut = substr(cut, 1, 1))

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, cut := substr(cut, 1, 1)]
diamonds_dt2

diamonds_tbl %>%
  mutate(carat = factor(carat))

diamonds_dt2[, carat := factor(carat)]
diamonds_dt2
rm(diamonds_dt2)
```

Here are some more ways to create new variables. Translating to `data.table` is trivial:

```{r}
diamonds_tbl %>%
  mutate(carat = factor(ntile(carat, 5)))
diamonds_tbl %>%
  mutate(carat = percent_rank(carat))
diamonds_tbl %>%
  mutate(lag_price = lag(price)) #if this data was a time series
diamonds_tbl %>%
  mutate(cumul_price = cumsum(price)) #%>% tail
```

How about if you want to create a column and drop all other columns in the process?

```{r}
diamonds_tbl %>%
  transmute(volume = x * y * z) #adds a new column dropping the old ones

diamonds_dt[, .(volume = x * y * z)]
```

There are many ways to reshape a dataset. We will see two now and a few functions later when it becomes important. For instance: we can collapse columns together using the `unite` function from package `tidyr` (which should be loaded when you load `dplyr`). We will have a short unit on more exciting and useful reshapings later ("long" to "short" and vice-versa). As far as I know `data.table` has a less elegant... unless someone has a better idea?

```{r}
diamonds_tbl2 = diamonds_tbl %>%
  unite(dimensions, x, y, z, sep = " x ")
diamonds_tbl2

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, dimensions := paste(x, y, z, sep = " x ")] #mutating
diamonds_dt2 = diamonds_dt2[, !c("x", "y", "z")]
diamonds_dt2
```

We can reverse this operation:

```{r}
diamonds_tbl2 %>%
  separate(dimensions, c("x", "y", "z"), sep = " x ")
rm(diamonds_tbl2)

diamonds_dt2[, c("x", "y", "z") := strsplit(dimensions, "x")]
diamonds_dt2[, -"dimensions"]
rm(diamonds_dt2)
```

There are tons of other packages to do clever things. For instance, here's one that does dummies. Let's convert the color feature to dummies. Again slightly less readable or elegant in `data.table`:

```{r}
pacman::p_load(sjmisc, snakecase)
diamonds_tbl %>%
  to_dummy(color, suffix = "label") %>% #this creates all the dummies
  bind_cols(diamonds_tbl) %>% #now we have to add all the original data back in
  select(-matches("_"), everything()) %>% #this puts the dummies last
  select(-color) #finally we can drop color

cbind(
  diamonds_dt[, -"color"], 
  to_dummy(diamonds_dt[, .(color)], suffix = "label")
)
```


What if you want to create a new variable based on functions only run on subsets of the data. This is called "grouping". Grouping only makes sense for categorical variables. (If you group on a continuous variable, then chances are you'll have $n$ different groups because you'll have $n$ unique values).

For instance:

```{r}
diamonds_tbl %>%
  group_by(color)

diamonds_dt[,, by = color]
```

Nothing happened... these were directives to do things a bit differently with the addition of other logic. So after you group, you can now run operations on each group like they're their own sub-data frame. Usually, you want to *summarize* data by group. This means you take the entire sub-data frame and run one metric on it and return only those metrics (i.e. shrink $n$ rows to $L$ rows). This sounds more complicated than it is and it is where data wrangling really gets fun. 

Here are a few examples:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price))

diamonds_dt[, .(avg_price = mean(price)), by = color][order(color)] #chaining / piping [...][...][...] etc
#where did all the other rows and columns go???

diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price), sd_price = sd(price), count = n())

diamonds_dt[, .(avg_price = mean(price), sd_price = sd(price), count = .N), by = color][order(color)]

diamonds_tbl %>%
  group_by(color) %>%
  summarize(min_price = min(price), med_price = median(price), max_price = max(price))

diamonds_dt[, .(min_price = min(price), med_price = median(price), max_price = max(price)), by = color][order(color)]
```

Sometimes you want to do fancier things like actually run operations on the whole sub-data frame using `mutate`. If the function is a single metric, then that metric is then duplicated across the whole sub data frame.

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(avg_price_for_color = mean(price))
#creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, avg_price_for_color := mean(price), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

So that's kind of like duplicating a summary stat. Here's something more fun: actually creating a new vector:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(price_rank_within_color = dense_rank(price)) #creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, price_rank_within_color := frankv(price, ties.method = "dense"), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

What if we want to get the first row in each category?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1)

diamonds_dt[, .SD[1], by = color][order(color)]
```

The `.SD` variable is short for "sub dataframe" and it's a stand-in for the pieces of the dataframe for each color as it loops over the colors. So `.SD[1]` will be first row in the sub dataframe. The reason why the matrices come out different is that the order of the rows in data.table changes based on optimizations. We'll see some of this later. I'm also unsure why it moved the `color` column to the front.

What about first and last?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1, n())

diamonds_dt[, .SD[c(1, .N)], by = color]
```

How about the diamond with the highest price by color?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  arrange(price) %>%
  slice(n())

diamonds_dt[, .SD[which.max(price)], by = color]
```

We've seen `data.table`'s preference for mutating functions. Here is a pipe command from package `magrittr` that makes the functions mutating. 

```{r}
diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 = diamonds_tbl2 %>%
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2

diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 %<>% #pipe and overwrite (short for what's above)
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2
rm(diamonds_tbl2)
```

This is as far we will go with data wrangling right now.

Let's benchmark a few core features of both packages. To do so, let's create a dataframe that's very big:

```{r}
pacman::p_load(microbenchmark)

Nbig = 2e6
diamonds_tbl_big = diamonds_tbl %>%
  sample_n(Nbig, replace = TRUE)
diamonds_dt_big = data.table(diamonds_tbl_big) #just to make sure we have the same data
diamonds_big = data.frame(diamonds_tbl_big) #ensure that it is a base R object
```

How about we write this dataframe to the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = write.csv(diamonds_big, "diamonds_big.csv"),
  tidyverse = write_csv(diamonds_tbl_big, "diamonds_big.csv"),
  data.table = fwrite(diamonds_dt_big, "diamonds_big.csv"),
    times = 1
)
```

How about we read this dataframe from the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = read.csv("diamonds_big.csv"),
  tidyverse = read_csv("diamonds_big.csv"),
  data.table = fread("diamonds_big.csv"),
    times = 1
)
```

What about for creating new variables?

```{r}
microbenchmark(
  base_R = {diamonds_big$log_price = log(diamonds_big$price)},
  tidyverse = {diamonds_tbl_big %<>% mutate(log_price = log(price))},
  data.table = diamonds_dt_big[, log_price := log(price)],
    times = 10
)
```

About the same. How about grouping and summarizing? No easy one-liner in base R. So we just compare the two packages:


```{r}
microbenchmark(
  tidyverse = {diamonds_tbl_big %>% group_by(color) %>% summarize(avg_price = mean(price))},
  data.table = diamonds_dt_big[, .(avg_price = mean(price), by = color)],
    times = 10
)
```

How about sorting?

```{r}
microbenchmark(
  base_R = diamonds_big[order(diamonds_big$price), ],
  tidyverse = {diamonds_tbl_big %>% arrange(price)},
  data.table = diamonds_dt_big[order(price)],
    times = 10
)
```
How about filtering?

```{r}
microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 10
)
```

Let's do this again but first "key" the price column which is what you would do if you are doing lots of searches.

```{r}
setkey(diamonds_dt_big, price)

microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 30
)
```


# Wide and Long Dataframe Formats

Another one of the core data munging skills is to transform a data frame from wide to long (most common) and from long back to wide (less common). Let's learn this by way of example.

```{r}
pacman::p_load(data.table, tidyverse, magrittr)
summary(storms)
head(storms)
```

Let's first create a few variables that are of interest:

```{r}
storms %<>% 
  mutate(wind_pct_avg = wind / mean(wind, na.rm = TRUE) * 100) %>%
  mutate(pressure_pct_avg = pressure / mean(pressure, na.rm = TRUE) * 100) %>%
  mutate(ts_diameter_pct_avg = ts_diameter / mean(ts_diameter, na.rm = TRUE) * 100) %>%
  mutate(hu_diameter_pct_avg = hu_diameter / mean(hu_diameter, na.rm = TRUE) * 100)
ggplot(storms) + 
  aes(wind_pct_avg) + 
  geom_histogram()
```

Now let's take a look at these four variables we created for a storm we all remember and create a time period variable. I'll also instantiate a data.table object for later:

```{r}
sandy_wide_tbl = storms %>% 
  filter(name == "Sandy") %>%
  select(wind_pct_avg, pressure_pct_avg, ts_diameter_pct_avg, hu_diameter_pct_avg) %>% #we only care about our variables
  mutate(period = 1 : n()) %>%
  select(period, everything()) #reorder
sandy_wide_dt = data.table(sandy_wide_tbl)
sandy_wide_dt
```

This is called a "repeated measures" dataset or a "time series" and it is one of the most common data frame types. Unfortunately, we didn't have enough classtime to do a unit on time series. It really deserves its own class!

Regardless, it would be nice to be able to visualize It would be nice to look at the four variables we just created by time period. We can do this below:

```{r}
ggplot(sandy_wide_tbl) + 
  aes(x = period) + 
  geom_line(aes(y = wind_pct_avg), col = "red") + 
  geom_line(aes(y = pressure_pct_avg), col = "green") + 
  geom_line(aes(y = ts_diameter_pct_avg), col = "blue") + 
  geom_line(aes(y = hu_diameter_pct_avg), col = "grey") +
  #make legend code
  ylab("% over average")
```

Notice how that was a lot of lines of code which aren't so maintainable and we don't have a legend. Legends are built automatically in `ggplot2` when we set color to a variable. This means we somehow have to let the four variables we care about be there own categorical variable.

First note that the dataframe we have is in what's called "wide format" or "unstacked" meaning each row is an observation and the columns are its features. This is exactly the format of dataframe that we've been studying in this class. This is the format we humans prefer to read and it is the format for many important analyses and the format for modeling.

However, to get what we want above involves a "reshaping" our dataframe into another canonical form, one that is easier for machines to read, a format called "long format" or "narrow" or "stacked" which looks like this:

| Period      | Value       | variable     |
| ----------- | ----------- | -------------|
| 1           | 56.08       | wind_pct_avg |
| 2           | 65.43       | wind_pct_avg |
etc.

Sometimes this format is required for situations, so we should get used to "pivoting" between the two formats. 

We first go from wide to long. To do so, we identify the "id variables" which get their own row per category and the measurement variables which get their own entire subdataframe.

```{r}
sandy_long_tbl = pivot_longer(
  sandy_wide_tbl, 
  cols = -period, #measurement variables: all column except period and period is then the ID variable
  names_to = "metric", #default is "name"
  values_to = "val" #default is "value"
)
sandy_long_dt = melt(
  sandy_wide_dt,
  id.vars = "period",
  measure.vars = c("wind_pct_avg", "pressure_pct_avg", "ts_diameter_pct_avg", "hu_diameter_pct_avg"),
  variable.name = "metric",
  value.name = "val"
)
sandy_long_tbl
sandy_long_dt
```

Same output but note the difference in sorting: `tidyverse` sorts on the id variables first and `data.table` sorts on the measurements i.e. cbinding the subdataframes.

Now that it's in long format, the visualization code becomes very simple:

```{r}
ggplot(sandy_long_dt) +
  geom_line(aes(x = period, y = val, color = metric)) +
  ylab("% over average")
```

Now we go from long to wide:

```{r}
sandy_wide_tbl2 = pivot_wider(
  sandy_long_tbl,
  id_cols = period, 
  names_from = metric,
  values_from = val
)
sandy_wide_dt2 = dcast(
  sandy_long_dt,
  period ~ metric, #lhs is id and rhs is measurement variables
  value.var = "val" #the function can guess "val" has to be the cell values so it's not needed
)
sandy_wide_tbl2
sandy_wide_dt2
```

Who's faster?

```{r}
pacman::p_load(microbenchmark)
microbenchmark(
  wide_to_long_tidy = pivot_longer(
    sandy_wide_tbl, 
    cols = -period,
    names_to = "metric",
    values_to = "val"
  ),
  wide_to_long_dt = melt(
    sandy_wide_dt,
    id.vars = "period",
    measure.vars = c("wind_pct_avg", "pressure_pct_avg", "ts_diameter_pct_avg", "hu_diameter_pct_avg"),
    variable.name = "metric",
    value.name = "val"
  ),
  long_to_wide_tidy = pivot_wider(
    sandy_long_tbl,
    id_cols = period, 
    names_from = metric,
    values_from = val
  ),
  long_to_wide_dt = dcast(
    sandy_long_dt,
    period ~ metric,
    value.var = "val"
  ),
  times = 50
)
```

Looks like ``data.table::melt`` is 60x faster than tidyverse's pivot and ``data.tabe::dcast` is 2x faster than tidyverse's pivot.


# Joins

Another one of the core data munging skills is joining data frames together. In the real world, databases consist of multiple dataframes called "tables" and design matrices are built by gathering data from among many tables. To illustrate this, we load two datasets from the package `nycflights13`, one dataset about weather and one about airports:

```{r}
pacman::p_load(nycflights13, data.table, tidyverse, magrittr)
data(weather)
summary(weather)
data(airports)
summary(airports)

head(weather)
head(airports)
```

Note how the weather and airports datasets contain a common feature: name of airport. It is called `FAA` in airports and `origin` in weather.

First we rename the column in weather to match the column in airports:

```{r}
weather %<>% 
  rename(faa = origin)
```

We also pare down the datasets so we can see the joins more clearly:

```{r}
airports %<>% 
  select(faa, lat, lon)
weather %<>% 
  select(faa, time_hour, temp, humid, wind_speed, pressure, wind_gust)
head(airports)
head(weather)
airports_dt = data.table(airports)
weather_dt = data.table(weather)
```

Some features just aren't measured that often e.g. `wind_gust`.

Let's do some joins. First "left". This is likely the most common because it's usually how we conceptualize what we're doing in our heads.

```{r}
airports_and_weather = left_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)


airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa", all.x = TRUE)
airports_and_weather_dt = merge(airports_dt, weather_dt, all.x = TRUE) #note this works too since it knows faa is the only column in common but not recommended since specifying "by" is more clear
airports_and_weather_dt[sample(1 : .N, 500)]
```

Now "right"

```{r}
airports_and_weather = right_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)
airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa", all.y = TRUE)
airports_and_weather_dt = merge(airports_dt, weather_dt, all.y = TRUE)
airports_and_weather_dt[sample(1 : .N, 500)]
```


```{r}
airports_and_weather = inner_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)
airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa")
airports_and_weather_dt = merge(airports_dt, weather_dt)
airports_and_weather_dt[sample(1 : .N, 500)]
```

And full, keeping all the rows. We use a subset to show how this works:

```{r}
airports_without_EWR = airports %>%
  filter(faa != "EWR")
airports_without_EWR_dt = data.table(airports_without_EWR)
airports_without_EWR_and_weather = full_join(airports_without_EWR, weather, by = "faa")
airports_without_EWR_and_weather %>% sample_n(500)
airports_without_EWR_and_weather_dt = merge(airports_without_EWR_dt, weather_dt, by = "faa", all = TRUE)
airports_without_EWR_and_weather_dt = merge(airports_without_EWR_dt, weather_dt, all = TRUE)
airports_without_EWR_and_weather_dt[sample(.N, 500)]
```

There is also `semi_join` and `anti_join` that do the opposite of joining. In my experience, these use cases are limited.

