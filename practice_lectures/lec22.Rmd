---
title: "Practice Lecture 22 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


# Boosting?

Nice simple explanation: https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725

```{r}
pacman::p_load(xgboost)
```

Look at performance on adult dataset. This demo isn't working yet.

```{r}
xgboost_mod = xgboost(data = data.matrix(X_train), 
 label = as.numeric(y_train == ">50K"), 
 num_class = 2, #y = 0 or 1
 eta = 0.3, #default
 max_depth = 6, #default 
 nrounds = 25, 
 subsample = 1, #default 
 colsample_bytree = 1,
 eval_metric = "merror",
 objective = "multi:softmax",
 nthread = 3
)

y_hat_test = as.numeric(predict(xgboost_mod, data.matrix(X_test)))
oos_confusion = table(y_test, ifelse(y_hat_test == 0, "<=50K", ">50K"))
oos_confusion
cat("FDR =", oos_confusion[1, 2] / sum(oos_confusion[, 2]), "\n")
cat("FOR =", oos_confusion[2, 1] / sum(oos_confusion[, 1]), "\n")
```

Invert train and test splits and let it train on 28,000 observations.

```{r}
xgboost_mod = xgboost(data = data.matrix(X_test), 
 label = as.numeric(y_test == ">50K"), 
 num_class = 2, #y = 0 or 1
 eta = 0.5, #default
 max_depth = 6, #default 
 nrounds = 50, 
 subsample = 1, #default 
 colsample_bytree = 1,
 eval_metric = "merror",
 objective = "multi:softmax",
 nthread = 3
)

y_hat_test = as.numeric(predict(xgboost_mod, data.matrix(X_train)))
oos_confusion = table(y_train, ifelse(y_hat_test == 0, "<=50K", ">50K"))
oos_confusion
cat("FDR =", oos_confusion[1, 2] / sum(oos_confusion[, 2]), "\n")
cat("FOR =", oos_confusion[2, 1] / sum(oos_confusion[, 1]), "\n")
```

Lightning fast on $n = 30000$!!! But only a bit more accurate. This leads me to think the error is mostly in delta.

We can try playing with the hyperparams. A lot to CV over! You think this is a lot... wait until you see deep learning networks!



