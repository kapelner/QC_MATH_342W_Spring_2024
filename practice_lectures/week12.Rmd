---
title: "Practice Lectures Week 12 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "May 2, 2022"
---


# Logistic Regression for Binary Response

Let's clean up and load the cancer dataset, remove missing data, remove the ID column and add more appropriate feature names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've done this awhile ago!!!
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
head(biopsy$class)
```

We can either estimate probability of the biopsy tissue being benign (this would mean y = 1 is the benign category level) or estimate the probability of the biopsy tissue being malignant (this would mean y = 1 is the malignant category level).

Let's go with the latter. To make the encoding explicitly 0/1, we can cast the factor to numeric or we can rely on R's default factor representation i.e. that the first level is 0 and the second level is 1. Here, we can use this default without reordering since the levels above show that benign is first and thus  = 0 and malignant is second and thus = 1 (via coincidence of alphabetical order).

Now let's split into training and test for experiments:

```{r}
set.seed(1984)
K = 5
test_prop = 1 / K
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```


Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to set the family parameter to be "binomial" which means we are using the independent Bernoulli and within the binomial family, we are using the "logit" link. There are other types of family models we won't get a chance to study e.g. Poisson, negative binomial for count models

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = binomial(link = "logit"))
```

That was fast! There was actually a lot of optimization in that line. Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit the log odds of malignancy increases by 0.597...

All of the coefficients are positive which means if any of the covariates increase...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malginant. If you can read log odds, you'll see ... has a small change of being malignant and ... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

It's very sure of itself! 

Let's see $y$ by $\hat{p}$ another way:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there in the training set! How about the test set?

```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = y_test), alpha = 0.5)
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Looks pretty good! 

We now will talk about error metrics for probabilistic estimation models. That will give us a way to validate this model and provide an estimate of future performance. 

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Yup can't do arithmetic operations on factors. So now we have to go ahead and cast.

```{r}
y_train_binary = ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_binary - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on. And the oos Brier score?

```{r}
y_test_binary = ifelse(y_test == "malignant", 1, 0)
mean(-(y_test_binary - p_hats_test)^2)
```

Not as good but still very good!

What is the in-sample log score?

```{r}
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
```

This isn't bad (if you get intuition in reading them). And oos?

```{r}
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Not as good but still very good!

If we wanted to be more careful, we can use K-fold CV to get a less variable oos metric. Maybe we'll do that in a lab?


# Probit and Cloglog probability estimation

These are different generalized linear models but fit using the same code. All we need to do is change the link argument. For a probit regression we just do:

```{r}
probit_mod = glm(class ~ ., biopsy_train, family = binomial(link = "probit"))
```

This is complaining about numerical underflow or overflow. If you get a z-score that's really large in magnitude, then it says probability is 1 (if z score is positive) or 0 (if z score is negative)

```{r}
coef(probit_mod)
```

As we saw before, all coefficients for the covariates are positive. What's the interpretation of b for bare_nuclei?

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(probit_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

This is basically the same. How about out of sample?


```{r}
p_hats_test = predict(probit_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Also basically the same-looking. To get an apples-apples comparison with logistic regression let's calculate the brier and log scoring metrics:

```{r}
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

It appears the logistic regression is better oos.

Let's do complementary log-log too:

```{r}
cloglog_mod = glm(class ~ ., biopsy_train, family = binomial(link = "cloglog"))
coef(cloglog_mod)
```

Same signs on coefficients. Interpretation? Difficult... 

Let's see how it does compared to the logistic and probit models.

```{r}
p_hats_train = predict(cloglog_mod, biopsy_train, type = "response")
p_hats_test = predict(cloglog_mod, biopsy_test, type = "response")
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Much worse than either! 

Logistic regression is usually the default. But just because it's the default and most popular and just because it won here doesn't mean it will always win!! Using probit or any other link function constitutes a completely different model. You can use the "model selection procedure" we discussed before to pick the best one.


```{r}
rm(list = ls())
```

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year in 1994 dollars. That's the equivalent of almost $90K/yr today (see https://www.in2013dollars.com/us/inflation/1994?amount=50000).

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
adult$fnlwgt = NULL
adult$occupation = NULL
adult$native_country = NULL
```

Let's use samples of 5,000 to run experiments:

```{r}
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial") #shortcut for binomial(link = "logit")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
length(coef(logistic_mod))
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)), alpha = 0.5)
```

Much more humble!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and usually confident about the small incomes.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
y_train_numeric = ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is a decent Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyone understands classification errors!


# Using Probability Estimation to do Classification

First repeat quickly (a) load the adult data (b) do a training / test split and (c) build the logisitc model.

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness

set.seed(1)
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

logistic_mod = glm(income ~ ., adult_train, family = "binomial")
p_hats_train = predict(logistic_mod, adult_train, type = "response")
p_hats_test = predict(logistic_mod, adult_test, type = "response")
```

Let's establish a rule: if the probability estimate is greater than or equal to 50%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.5, ">50K", "<=50K"))
```

How did this "classifier" do in-sample?

```{r}
mean(y_hats_train != y_train)
in_sample_conf_table = table(y_train, y_hats_train)
in_sample_conf_table
```
And the performance stats:


```{r}
n = sum(in_sample_conf_table)
fp = in_sample_conf_table[1, 2]
fn = in_sample_conf_table[2, 1]
tp = in_sample_conf_table[2, 2]
tn = in_sample_conf_table[1, 1]
num_pred_pos = sum(in_sample_conf_table[, 2])
num_pred_neg = sum(in_sample_conf_table[, 1])
num_pos = sum(in_sample_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

That was in-sample which may be overfit. Howe about oos?

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.5, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

A tad bit worse. Here are estimates of the future performance for each class:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```
Worse than in-sample (expected). But still could be "good enough" depending on your definition of "good enough".

However... this whole classifier hinged on the decision of the prob-threshold = 50%! What if we change this default threshold??

# Asymmetric Cost Classifiers

Let's establish a *new* rule: if the probability estimate is greater than or equal to 90%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.9, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

Of course the misclassification error went up! But now look at the confusion table! The second column represents all $\hat{y} = 1$ and there's not too many of them! Why? You've made it *much* harder to classify something as positive. Here's the new additional performance metrics now:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

We don't make many false discoveries but we make a lot of false omissions! It's a tradeoff...


# Receiver-Operator Curve Plot

The entire classifier is indexed by that indicator function probability threshold which creates the classification decision. Why not see look at the entire range of possible classification models. We do this with a function. We will go through it slowly and explain each piece:

```{r}
#' Computes performance metrics for a binary probabilistic classifer
#'
#' Each row of the result will represent one of the many models and its elements record the performance of that model so we can (1) pick a "best" model at the end and (2) overall understand the performance of the probability estimates a la the Brier scores, etc.
#'
#' @param p_hats  The probability estimates for n predictions
#' @param y_true  The true observed responses
#' @param res     The resolution to use for the grid of threshold values (defaults to 1e-3)
#'
#' @return        The matrix of all performance results
compute_metrics_prob_classifier = function(p_hats, y_true, res = 0.001){
  #we first make the grid of all prob thresholds
  p_thresholds = seq(0 + res, 1 - res, by = res) #values of 0 or 1 are trivial
  
  #now we create a matrix which will house all of our results
  performance_metrics = matrix(NA, nrow = length(p_thresholds), ncol = 12)
  colnames(performance_metrics) = c(
    "p_th",
    "TN",
    "FP",
    "FN",
    "TP",
    "miscl_err",
    "precision",
    "recall",
    "FDR",
    "FPR",
    "FOR",
    "miss_rate"
  )
  
  #now we iterate through each p_th and calculate all metrics about the classifier and save
  n = length(y_true)
  for (i in 1 : length(p_thresholds)){
    p_th = p_thresholds[i]
    y_hats = factor(ifelse(p_hats >= p_th, ">50K", "<=50K"))
    confusion_table = table(
      factor(y_true, levels = c("<=50K", ">50K")),
      factor(y_hats, levels = c("<=50K", ">50K"))
    )
      
    fp = confusion_table[1, 2]
    fn = confusion_table[2, 1]
    tp = confusion_table[2, 2]
    tn = confusion_table[1, 1]
    npp = sum(confusion_table[, 2])
    npn = sum(confusion_table[, 1])
    np = sum(confusion_table[2, ])
    nn = sum(confusion_table[1, ])
  
    performance_metrics[i, ] = c(
      p_th,
      tn,
      fp,
      fn,
      tp,
      (fp + fn) / n,
      tp / npp, #precision
      tp / np,  #recall
      fp / npp, #false discovery rate (FDR)
      fp / nn,  #false positive rate (FPR)
      fn / npn, #false omission rate (FOR)
      fn / np   #miss rate
    )
  }
  
  #finally return the matrix
  performance_metrics
}
```

Now let's generate performance results for the in-sample data:

```{r}
pacman::p_load(data.table, magrittr)
performance_metrics_in_sample = compute_metrics_prob_classifier(p_hats_train, y_train) %>% data.table
performance_metrics_in_sample
```

Now let's plot the ROC curve

```{r}
pacman::p_load(ggplot2)
ggplot(performance_metrics_in_sample) +
  geom_line(aes(x = FPR, y = recall)) +
  geom_abline(intercept = 0, slope = 1, col = "orange") + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1)
```

Now calculate the area under the curve (AUC) which is used to evaluate the probabilistic classifier (just like the Brier score) using a trapezoid area function. 

```{r}
pacman::p_load(pracma)
-trapz(performance_metrics_in_sample$FPR, performance_metrics_in_sample$recall)
```

This is not bad at all!

Note that I should add $<0, 0>$ and $<1, 1>$ as points before this is done but I didn't...

How do we do out of sample?


```{r}
performance_metrics_oos = compute_metrics_prob_classifier(p_hats_test, y_test) %>% data.table
performance_metrics_oos
```

And graph the ROC:


```{r}
#first we do our own melting of two data frames together to make it long format
performance_metrics_in_and_oos = rbind(
    cbind(performance_metrics_in_sample, data.table(sample = "in")),
    cbind(performance_metrics_oos, data.table(sample = "out"))
)
ggplot(performance_metrics_in_and_oos) +
  geom_line(aes(x = FPR, y = recall, col = sample)) +
  geom_abline(intercept = 0, slope = 1, col = "orange") + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1)
```


```{r}
-trapz(performance_metrics_oos$FPR, performance_metrics_oos$recall)
```


Not bad at all - only a tad worse! In the real world it's usually a lot worse. We are lucky we have n = 5,000 in both a train and test set.

# Detection Error Tradeoff curve

```{r}
table(y_test) / length(y_test)
table(y_train) / length(y_train)
ggplot(performance_metrics_in_and_oos) +
  geom_line(aes(x = FDR, y = miss_rate, col = sample)) +
  coord_fixed() + xlim(0, 1) + ylim(0, 1)
```


#Using AUC to Compare Probabilistic Classification Models

What would the effect be of less information on the same traing set size? Imagine we didn't know the features: occupation, education, education_num, relationship, marital_status. How would we do relative to the above? Worse!

```{r}
pacman::p_load(data.table, tidyverse, magrittr)
if (!pacman::p_isinstalled(ucidata)){
  pacman::p_load_gh("coatless/ucidata")
} else {
  pacman::p_load(ucidata)
}

data(adult)
adult = na.omit(adult) #kill any observations with missingness

set.seed(1)
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

logistic_mod_full = glm(income ~ ., adult_train, family = "binomial")
p_hats_test = predict(logistic_mod_full, adult_test, type = "response")

performance_metrics_oos_full_mod = compute_metrics_prob_classifier(p_hats_test, y_test) %>% data.table

logistic_mod_red = glm(income ~ . - occupation - education - education_num - relationship - marital_status, adult_train, family = "binomial")
p_hats_test = predict(logistic_mod_red, adult_test, type = "response")

performance_metrics_oos_reduced_mod = compute_metrics_prob_classifier(p_hats_test, y_test) %>% data.table


ggplot(rbind(
  performance_metrics_oos_full_mod[, model := "full"],
  performance_metrics_oos_reduced_mod[, model := "reduced"]
)) +
  geom_point(aes(x = FPR, y = recall, shape = model, col = p_th), size = 1) +
  geom_abline(intercept = 0, slope = 1) + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1) + 
  scale_colour_gradientn(colours = rainbow(5))
```

and we can see clearly that the AUC is worse. This means that the full model dominates the reduced model for every FPR / TPR pair.

```{r}
pacman::p_load(pracma)
-trapz(performance_metrics_oos_reduced_mod$FPR, performance_metrics_oos_reduced_mod$recall)
-trapz(performance_metrics_oos_full_mod$FPR, performance_metrics_oos_full_mod$recall)
```

As we lose information that is related to the true causal inputs, we lose predictive ability. Same story for this entire data science class since error due to ignorance increases! And certainly no different in probabilistic classifiers.

Here's the same story with the DET curve:

```{r}
ggplot(rbind(
  performance_metrics_oos_full_mod[, model := "full"],
  performance_metrics_oos_reduced_mod[, model := "reduced"]
)) +
  geom_point(aes(x = FDR, y = miss_rate, shape = model, col = p_th), size = 1) +
  coord_fixed() + xlim(0, 1) + ylim(0, 1) + 
  scale_colour_gradientn(colours = rainbow(5))
```


# Choosing a Decision Threshold Based on Asymmetric Costs and Rewards

The ROC and DET curves gave you a glimpse into all the possible classification models derived from a probability estimation model. Each point on that curve is a separate $g(x)$ with its own performance metrics. How do you pick one?

Let's create rewards and costs. Imagine we are trying to predict income because we want to sell people an expensive item e.g. a car. We want to advertise our cars via a nice packet in the mail. The packet costs \$5. If we send a packet to someone who really does make $>50K$/yr then we are expected to make \$1000. So we have rewards and costs below:

```{r}
r_tp = 0
c_fp = -5
c_fn = -1000
r_tn = 0
```

Let's return to the linear logistic model with all features. Let's calculate the overall oos average reward per observation (per person) for each possible $p_{th}$:

```{r}
n = nrow(adult_test)
performance_metrics_oos_full_mod$avg_cost = 
  (r_tp * performance_metrics_oos_full_mod$TP +
  c_fp * performance_metrics_oos_full_mod$FP +
  c_fn * performance_metrics_oos_full_mod$FN +
  r_tn * performance_metrics_oos_full_mod$TN) / n
```

Let's plot average reward (reward per person) by threshold:

```{r}
ggplot(performance_metrics_oos_full_mod) +
  geom_line(aes(x = p_th, y = avg_cost)) #+
  # xlim(0, 0.05) + ylim(-5,0)
```

Obviously, the best decision is $p_{th} \approx 0$ which means you classifiy almost everything as a positive. This makes sense because the mailing is so cheap. What are the performance characteristics of the optimal model?

```{r}
i_star = which.max(performance_metrics_oos_full_mod$avg_cost)
performance_metrics_oos_full_mod[i_star, ]

# performance_metrics_oos_full_mod[, .(p_th, avg_cost)]
```

The more interesting problem is where the cost of advertising is higher:

```{r}
r_tp = 0
c_fp = -200
c_fn = -1000
r_tn = 0
performance_metrics_oos_full_mod$avg_cost = 
  (r_tp * performance_metrics_oos_full_mod$TP +
  c_fp * performance_metrics_oos_full_mod$FP +
  c_fn * performance_metrics_oos_full_mod$FN +
  r_tn * performance_metrics_oos_full_mod$TN) / n
ggplot(performance_metrics_oos_full_mod) +
  geom_point(aes(x = p_th, y = avg_cost), lwd = 0.01)
```

What are the performance characteristics of the optimal model?

```{r}
i_star = which.max(performance_metrics_oos_full_mod$avg_cost)
performance_metrics_oos_full_mod[i_star, ]
```

If $g_{pr}$ is closer to $f_{pr}$, what happens? 

All the threshold-derived classification models get better and you are guaranteed to make more money since you have a better discriminating eye.

There is also a way to make asymmetric models with trees. We may do this later if we have time.


#Classification Trees and Confusion Tables

Let's load up the adult dataset where the response is 1 if the person makes more than $50K per year and 0 if they make less than $50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult %<>% 
  na.omit #kill any observations with missingness
```

Let's use samples of 2,000 to run experiments:

```{r}
train_size = 2000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
test_indices = sample(setdiff(1 : nrow(adult), train_indices), train_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
tree_mod
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

Compute in-sample and out of sample fits:

```{r}
y_hat_train = predict(tree_mod, X_train)
y_hat_test = predict(tree_mod, X_test)
```

Let's look at the confusion table in-sample:

```{r}
table(y_train, y_hat_train)
```

There are no errors here! Thus, precision and recall are both 100%. This makes sense because classification trees overfit.

Let's do the same oos:

```{r}
oos_conf_table = table(y_test, y_hat_test)
oos_conf_table
```

We didn't do as well (of course). Let's calculate some performance metrics. We assume ">50k" is the "positive" category and "<=50k" is the "negative" category. Note that this choice is arbitrary and everything would just be switched if we did it the other way.

```{r}
n = sum(oos_conf_table)
n
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
acc = (tp + tn) / n
acc
misclassifcation_error = 1 - acc
misclassifcation_error
precision = tp / num_pred_pos
precision
recall = tp / num_pos
recall
false_discovery_rate = 1 - precision
false_discovery_rate
false_omission_rate = fn / num_pred_neg
false_omission_rate
```

Let's see how this works on a dataset whose goal is classification for more than 2 levels. Note: this is only possible now with trees!

```{r}
rm(list = ls())
pacman::p_load(mlbench, skimr)
data(LetterRecognition)
LetterRecognition = na.omit(LetterRecognition) #kill any observations with missingness
skim(LetterRecognition)
?LetterRecognition
```

Now we split the data:

```{r}
test_samp = 500
train_indices = sample(1 : nrow(LetterRecognition), test_samp)
ltr_train = LetterRecognition[train_indices, ]
y_train = ltr_train$lettr
X_train = ltr_train
X_train$lettr = NULL
test_indices = sample(setdiff(1 : nrow(LetterRecognition), train_indices), test_samp)
ltr_test = LetterRecognition[test_indices, ]
y_test = ltr_test$lettr
X_test = ltr_test
X_test$lettr = NULL
```

And fit a tree model and its in-sample and oos fits:

```{r}
tree_mod = YARFCART(X_train, y_train)
y_hat_train = predict(tree_mod, X_train)
y_hat_test = predict(tree_mod, X_test)
```

Take a look at the in-sample confusion matrix:

```{r}
table(y_train, y_hat_train)
```

Perfecto... as expected... 

Now the oos confusion matrix:

```{r}
oos_confusion_table = table(y_test, y_hat_test)
oos_confusion_table
```

Hard to read. Let's make it easier to read by blanking out the diagonal and looking at entried only >= 5:

```{r}
oos_confusion_table[oos_confusion_table < 2] = ""
diag(oos_confusion_table) = "."
oos_confusion_table
mean(y_test != y_hat_test)
```

What's it using to determine letter?

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
```

Where did these features comes from?? Deep learning helps to create the features from the raw pixel data. Wish I had a whole next semester to discuss this...

Random Forests:

```{r}
num_trees = 500
train_size = 2000

training_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[training_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
```

And test:

```{r}
test_indices = sample(setdiff(1 : nrow(adult), training_indices), 25000)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

y_hat_test_bag = predict(mod_bag, X_test)
y_hat_test_rf = predict(mod_rf, X_test)

oos_conf_table_bag = table(y_test, y_hat_test_bag)
oos_conf_table_rf = table(y_test, y_hat_test_rf)
oos_conf_table_bag
oos_conf_table_rf
miscl_err_bag = mean(y_test != y_hat_test_bag)
miscl_err_rf = mean(y_test != y_hat_test_rf)
miscl_err_bag
miscl_err_rf

cat("gain: ", (miscl_err_bag - miscl_err_rf) / miscl_err_bag * 100, "%\n")
```

And on letters:

```{r}
test_samp = 2000
train_indices = sample(1 : nrow(LetterRecognition), test_samp)
ltr_train = LetterRecognition[train_indices, ]
y_train = ltr_train$lettr
X_train = ltr_train
X_train$lettr = NULL
test_indices = sample(setdiff(1 : nrow(LetterRecognition), train_indices), test_samp)
ltr_test = LetterRecognition[test_indices, ]
y_test = ltr_test$lettr
X_test = ltr_test
X_test$lettr = NULL
```

And fit a tree model and its in-sample and oos fits:

```{r}
mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_bag
mod_rf

y_hat_test_bag = predict(mod_bag, X_test)
y_hat_test_rf = predict(mod_rf, X_test)

oos_conf_table_bag = table(y_test, y_hat_test_bag)
oos_conf_table_rf = table(y_test, y_hat_test_rf)
oos_conf_table_bag
oos_conf_table_rf
miscl_err_bag = mean(y_test != y_hat_test_bag)
miscl_err_rf = mean(y_test != y_hat_test_rf)
miscl_err_bag
miscl_err_rf

cat("gain: ", (miscl_err_bag - miscl_err_rf) / miscl_err_bag * 100, "%\n")
```

Very real gains for classification.



# Missingness

Take a look at an housing dataset from Australia:

https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/home?select=melb_data.csv#


```{r}
pacman::p_load(tidyverse, magrittr, data.table, skimr, R.utils)
apts = fread("melb_data.csv.bz2")
skim(apts)
```

We drop all character variables first just for expedience in the demo. If you were building a prediction model, you would scour them carefully to see if there is any signal in them you can use, and then mathematize them to metrics if so.

```{r}
apts %<>%
  select_if(is.numeric) %>%
  select(Price, everything())
```

Imagine we were trying to predict `Price`. So let's section our dataset:

```{r}
y = apts$Price
X = apts %>% 
  select(-Price)
rm(apts)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = as_tibble(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
M %<>% 
  select_if(function(x){sum(x) > 0})
head(M)
skim(M)
```

Some of these missing indicators might be collinear because they share all the rows they are missing on. Let's filter those out if they exist:

```{r}
M = as_tibble(t(unique(t(M))))
skim(M)
```

Without imputing and without using missingness as a predictor in its own right, let's see what we get with a basic linear model now:

```{r}
lin_mod_listwise_deletion = lm(y ~ ., X)
summary(lin_mod_listwise_deletion)
```

Not bad ... but this is only on the data that has full records! There are 6,750 observations dropped!

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 13,580 observations) so we will sample 2,000 observations for each of the trees. This is a typical strategy when fitting RF. It definitely reduces variance but increases bias. But we don't have a choice since we don't want to wait forever.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(2000, ncol(X)))$ximp
skim(Ximp)
```


Now we consider our imputed dataset as the design matrix.

```{r}
linear_mod_impute = lm(y ~ ., Ximp)
summary(linear_mod_impute)
```
We can do even better if we use all the information i.e. including the missingness. We take our imputed dataset, combine it with our missingness indicators for a new design matrix.

```{r}
Ximp_and_missing_dummies = data.frame(cbind(Ximp, M))
linear_mod_impute_and_missing_dummies = lm(y ~ ., Ximp_and_missing_dummies)
summary(linear_mod_impute_and_missing_dummies)
```

Not much gain, but there seems to be something.

Are these two better models than the original model that was built with listwise deletion of observations with missingness?? 

Are they even comparable? It is hard to compare the two models since the first model was built with only non-missing observations which may be easy to predict on and the second was built with the observations that contained missingness. Those extra 6,750 are likely more difficult to predict on. So this is complicated...

Maybe one apples-to-apples comparison is you can replace all the missingness in the original dataset with something naive e.g. the average and then see who does better. This at least keeps the same observations.

```{r}
X %<>% mutate(Rooms = as.numeric(Rooms))
Xnaive = X %>%
 replace_na(as.list(colMeans(X, na.rm = TRUE)))
linear_mod_naive_without_missing_dummies = lm(y ~ ., Xnaive)
summary(linear_mod_naive_without_missing_dummies)
```

There is a clear gain to imputing and using is_missing dummy features to reduce delta (55.3% vs 52.4% Rsqs).

Note: this is just an illustration of best practice. It didn't necessarily have to "work".

