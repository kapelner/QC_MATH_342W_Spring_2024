---
title: "Lab 11"
author: "Your Name Here"
output: pdf_document
---

#Boosting

We will make use of YARF so here' the boilerplate code.

```{r}
options(java.parameters = "-Xmx8000m")
pacman::p_load(rJava)
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
pacman::p_load(YARF)
```

We will now write a gradient boosting algorithm from scratch. We will make it as general as possible for regression and classification.

```{r}
pacman::p_load(checkmate) #this is a package that enforces arguments are the correct form

#' A general gradient boosting algorithm
#'
#' @param X                         A data frame representing the features. It is of size n x p. No need for an intercept column.
#' @param y                         A vector of length n. It either will be real numbers (for regression) or binary (for classification).
#' @param g_base_learner_alg        A function with arguments X, y and ... and returns a function that takes X as an argument. The default is YARFCART
#'                                  with nodesize 10% of the total length.
#' @param neg_grad_objective_function   The negative gradient of the function to be minimized. It takes arguments y, yhat that returns a vector. The default objective function is SSE for
#'                                  regression and logistic loss for classification.
#' @param M                         The number of base learners to be summed. Default is 50 for regression and 100 for classification.
#' @param eta                       The step size in the gradient descent. Default is 0.3
#' @param ...                       Optional arguments to be passed into the g_base_learner_alg function.
#'
#' @return
gradient_boost = function(X, y, g_base_learner_alg = NULL, neg_grad_objective_function = NULL, M = NULL, eta = 0.3, ...){
  assert_data_frame(X)
  n = nrow(X)
  assert_numeric(y)
  assert(length(y) == n)
  assert_function(g_base_learner_alg, args = c("X", "y"), null.ok = TRUE)
  assert_function(neg_grad_objective_function, args = c("y", "yhat"), null.ok = TRUE)
  assert_count(M, positive = TRUE, null.ok = TRUE)
  assert_numeric(eta, lower = .Machine$double.eps)
  
  if(is.null(g_base_learner_alg)){
    g_base_learner_alg = function(X, y){
      YARFCART(X, y, nodesize = round(.1 * n), calculate_oob_error = FALSE, bootstrap_indices = list(1 : nrow(X)), verbose = FALSE)
    }
  }
  
  if (identical(sort(names(table(y))), c("0", "1"))){
    #classification
    cat("building gradient boosted trees for probability estimation of two classes\n")
    if(is.null(M)){
      M = 100
    }
    if(is.null(neg_grad_objective_function)){
      neg_grad_objective_function = function(y, y_hat){
        y - exp(y_hat) / (1+exp(y_hat))
      }
    }
    g_0 = function(X_star){
      rep(exp(mean(y))/ (1 + exp(mean(y))), nrow(X_star)) # convert y_hat, which is in log_odds form, back to the probability  
    }
  } else {
    #regression
    cat("building gradient boosted trees for regression\n")
    if (is.null(M)){
      M = 50
    }
    if (is.null(neg_grad_objective_function)){
      neg_grad_objective_function = function(y, y_hat){
        2 * (y - y_hat)
      }
    }
    g_0 = function(X_star){
      rep(mean(y), nrow(X_star))
    }
  }
  #TO-DO
  g_tildes = list()
  y_hat_0 = g_0(X)
  for (m in 1 : M) {
    cat("fitting tree", m, "of", M, "\n")
    y_hat_m = y_hat_0
    if (m > 1){
     for (k in 1 : (m - 1)){
        y_hat_m = y_hat_m + eta * predict(g_tildes[[k]], X)
      }     
    }
    
    neg_gradient_m = neg_grad_objective_function(y, y_hat_m) # obtain negative gradient 
    g_tildes[[m]] = g_base_learner_alg(X, neg_gradient_m)
  }
  
  #now we return the final model which takes in a data.frame X_star and predicts with it
  function(X_star){
    assert_data_frame(X_star)
    
    y_hat_star = g_0(X_star)
    for (m in 1 : (M - 1)){
      y_hat_star = y_hat_star + eta * predict(g_tildes[[m]], X_star)
    } 
    y_hat_star
  }
}
```

Now we test the code in-sample:

```{r}
n = 100
p = 3
X = matrix(rnorm(n * p), nrow = n)
bbeta = seq(-1, 1, length.out = p)
y = c(X %*% bbeta + rnorm(n))
y_binary = rbinom(n, 1, 1 / (1 + exp(-X %*% bbeta)))
X = data.frame(X)

#regression
g_b = gradient_boost(X, y)
pacman::p_load(ggplot2)
ggplot(data.frame(y = y, yhat = g_b(X))) + aes(x = y, y = yhat) + geom_point()
sqrt(mean((y - g_b(X))^2)) #RMSE

#probability estimation
g_b = gradient_boost(X, y_binary)
table(y_binary, g_b(X) > 0)
```


Here is code to split up the diamonds dataset into three subsets:

```{r}
set.seed(1)
diamonds = ggplot2::diamonds
pacman::p_load(tidyverse)
diamonds = diamonds %>% 
  mutate(cut = factor(cut, ordered = FALSE)) %>%
  mutate(color = factor(color, ordered = FALSE)) %>%
  mutate(clarity = factor(clarity, ordered = FALSE))
diamonds_mm = model.matrix(price ~ ., diamonds)
train_size = 2000
train_indices = sample(1 : nrow(diamonds), train_size)

y_train = diamonds[train_indices, ]$price
X_train_mm = diamonds_mm[train_indices, ]

validation_size = 2000
validation_indices = sample(setdiff(1 : nrow(diamonds), train_indices), validation_size)
y_validation = diamonds[validation_indices, ]$price
X_validation_mm = diamonds_mm[validation_indices, ]

test_size = 2000
test_indices = sample(setdiff(1 : nrow(diamonds), c(train_indices, validation_indices)), test_size)
y_test = diamonds[test_indices, ]$price
X_test_mm = diamonds_mm[test_indices, ]
```

Using your new gradient boosting function, optimize the number of base learners, M for the diamonds data using a grid search:

```{r}
#TO-DO
```

Now find the error in the test set and comment on its performance:

```{r}
#TO-DO
```

Repeat this exercise for the adult dataset. First create the splits:

```{r}
#TO-DO
```

Using your new gradient boosting function, optimize the number of base learners, M for the diamonds data using a grid search:

```{r}
#TO-DO
```

Now find the error in the test set and comment on its performance:

```{r}
#TO-DO
```


