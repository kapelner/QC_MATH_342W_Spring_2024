% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/code.R
\name{gbm_fit}
\alias{gbm_fit}
\title{Gradient boosting}
\usage{
gbm_fit(
  X,
  y,
  g_base_learner_alg = NULL,
  neg_grad_objective_function = NULL,
  M = NULL,
  eta = 0.3,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{X}{A data frame representing the features. It is of size n x p. No need for an intercept column.}

\item{y}{A vector of length n. It either will be real numbers (for regression) or binary (for classification).}

\item{g_base_learner_alg}{A function with arguments X, y and ... and returns a function that takes X as an argument. The default is YARFCART
with nodesize 10\% of the total length.}

\item{neg_grad_objective_function}{The negative gradient of the function to be minimized. It takes arguments y, yhat that returns a vector. The default objective function is SSE for
regression and logistic loss for classification.}

\item{M}{The number of base learners to be summed. Default is 50 for regression and 100 for classification.}

\item{eta}{The step size in the gradient descent. Default is 0.3}

\item{verbose}{Messages are printed out during construction. Default is TRUE.}

\item{...}{Optional arguments to be passed into the g_base_learner_alg function.}
}
\value{
A "qc_basement_gbm" gradient boosting model which can be used for prediction
}
\description{
Generates a gradient boosting model based on your choices of base learner and objective function
}
\author{
Adam Kapelner
}
